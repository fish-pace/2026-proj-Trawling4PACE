{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: Install dependencies\n",
    "import sys, subprocess\n",
    "pkgs = [\"pandas\", \"numpy\", \"plotly\", \"ipywidgets\", \"kaleido\"]\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + pkgs + [\"--quiet\"])\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2: Imports\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üèÜ ML Model Comparison Dashboard\")\n",
    "print(\"Compare multiple ML models and analyze feature importance\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: Folder Browser\n",
    "\n",
    "class FolderBrowser:\n",
    "    \"\"\"Interactive folder browser widget.\"\"\"\n",
    "    \n",
    "    def __init__(self, start='.', label='Folder'):\n",
    "        self.cur = Path(start).resolve()\n",
    "        self.sel = self.cur\n",
    "        self.label = label\n",
    "        \n",
    "        self.html = widgets.HTML(f\"<code>{self.cur}</code>\")\n",
    "        self.dd = widgets.Select(\n",
    "            options=self._list(), \n",
    "            layout=widgets.Layout(width='100%', height='150px')\n",
    "        )\n",
    "        self.b_up = widgets.Button(description='‚Üë Up', layout=widgets.Layout(width='70px'))\n",
    "        self.b_in = widgets.Button(description='‚Üí Enter', layout=widgets.Layout(width='80px'))\n",
    "        self.b_sel = widgets.Button(description='‚úì Select', button_style='success', \n",
    "                                    layout=widgets.Layout(width='80px'))\n",
    "        self.selhtml = widgets.HTML(f\"<b>Selected:</b> <code>{self.sel}</code>\")\n",
    "        \n",
    "        self.b_up.on_click(lambda b: self._up())\n",
    "        self.b_in.on_click(lambda b: self._enter())\n",
    "        self.b_sel.on_click(lambda b: self._select())\n",
    "        \n",
    "        self.w = widgets.VBox([\n",
    "            widgets.HTML(f\"<b>{label}</b>\"),\n",
    "            self.html, self.dd,\n",
    "            widgets.HBox([self.b_up, self.b_in, self.b_sel]),\n",
    "            self.selhtml\n",
    "        ])\n",
    "    \n",
    "    def _list(self):\n",
    "        try:\n",
    "            items = ['.']\n",
    "            for x in sorted(self.cur.iterdir()):\n",
    "                if x.is_dir() and not x.name.startswith('.'):\n",
    "                    items.append(f\"üìÅ {x.name}\")\n",
    "            return items\n",
    "        except:\n",
    "            return ['.']\n",
    "    \n",
    "    def _refresh(self):\n",
    "        self.html.value = f\"<code>{self.cur}</code>\"\n",
    "        self.dd.options = self._list()\n",
    "    \n",
    "    def _up(self):\n",
    "        if self.cur.parent != self.cur:\n",
    "            self.cur = self.cur.parent\n",
    "            self._refresh()\n",
    "    \n",
    "    def _enter(self):\n",
    "        if self.dd.value and self.dd.value.startswith('üìÅ'):\n",
    "            folder_name = self.dd.value.replace('üìÅ ', '')\n",
    "            p = self.cur / folder_name\n",
    "            if p.is_dir():\n",
    "                self.cur = p\n",
    "                self._refresh()\n",
    "    \n",
    "    def _select(self):\n",
    "        self.sel = self.cur\n",
    "        self.selhtml.value = f\"<b>Selected:</b> <code>{self.sel}</code>\"\n",
    "    \n",
    "    def path(self):\n",
    "        return self.sel\n",
    "\n",
    "print(\"Folder browser defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4: Results Scanner Class\n",
    "\n",
    "class MLResultsScanner:\n",
    "    \"\"\"Scan and aggregate ML results from multiple model folders.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results_dir = None\n",
    "        self.models = {}  # {model_name: {metrics: df, importance: df, ...}}\n",
    "        self.metrics_df = None  # Combined metrics\n",
    "        self.importance_df = None  # Combined importance\n",
    "    \n",
    "    def scan(self, root_path: Path) -> Dict:\n",
    "        \"\"\"Scan directory for ML results.\"\"\"\n",
    "        self.results_dir = Path(root_path)\n",
    "        self.models = {}\n",
    "        \n",
    "        # Find all subdirectories with results\n",
    "        for subdir in self.results_dir.iterdir():\n",
    "            if not subdir.is_dir() or subdir.name.startswith('.'):\n",
    "                continue\n",
    "            \n",
    "            # Look for metrics and importance files\n",
    "            metrics_files = list(subdir.glob('*_metrics_*.csv'))\n",
    "            importance_files = list(subdir.glob('*_feature_importance_*.csv'))\n",
    "            summary_files = list(subdir.glob('*_summary_*.txt'))\n",
    "            prediction_files = list(subdir.glob('*_predictions_*.csv'))\n",
    "            \n",
    "            if metrics_files:\n",
    "                model_name = subdir.name\n",
    "                self.models[model_name] = {\n",
    "                    'path': subdir,\n",
    "                    'metrics_file': metrics_files[0] if metrics_files else None,\n",
    "                    'importance_file': importance_files[0] if importance_files else None,\n",
    "                    'summary_file': summary_files[0] if summary_files else None,\n",
    "                    'prediction_file': prediction_files[0] if prediction_files else None,\n",
    "                }\n",
    "        \n",
    "        return {\n",
    "            'n_models': len(self.models),\n",
    "            'models': list(self.models.keys())\n",
    "        }\n",
    "    \n",
    "    def load_all(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Load all metrics and importance data.\"\"\"\n",
    "        metrics_list = []\n",
    "        importance_list = []\n",
    "        \n",
    "        for model_name, info in self.models.items():\n",
    "            # Load metrics\n",
    "            if info['metrics_file']:\n",
    "                try:\n",
    "                    df = pd.read_csv(info['metrics_file'])\n",
    "                    df['model_folder'] = model_name\n",
    "                    metrics_list.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading metrics for {model_name}: {e}\")\n",
    "            \n",
    "            # Load importance\n",
    "            if info['importance_file']:\n",
    "                try:\n",
    "                    df = pd.read_csv(info['importance_file'])\n",
    "                    df['model_folder'] = model_name\n",
    "                    importance_list.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading importance for {model_name}: {e}\")\n",
    "        \n",
    "        # Combine\n",
    "        self.metrics_df = pd.concat(metrics_list, ignore_index=True) if metrics_list else None\n",
    "        self.importance_df = pd.concat(importance_list, ignore_index=True) if importance_list else None\n",
    "        \n",
    "        return self.metrics_df, self.importance_df\n",
    "    \n",
    "    def get_model_comparison(self) -> pd.DataFrame:\n",
    "        \"\"\"Create model comparison table.\"\"\"\n",
    "        if self.metrics_df is None:\n",
    "            return None\n",
    "        \n",
    "        # Pivot to get metrics by model\n",
    "        # Filter for test split only\n",
    "        test_metrics = self.metrics_df[self.metrics_df['split'] == 'test'].copy()\n",
    "        \n",
    "        # Pivot\n",
    "        comparison = test_metrics.pivot_table(\n",
    "            index='model_folder',\n",
    "            columns='metric',\n",
    "            values='value',\n",
    "            aggfunc='first'\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Rename columns for clarity\n",
    "        comparison.columns.name = None\n",
    "        \n",
    "        # Sort by R¬≤ descending\n",
    "        r2_col = [c for c in comparison.columns if 'R' in c and '2' in c or 'R¬≤' in c]\n",
    "        if r2_col:\n",
    "            comparison = comparison.sort_values(r2_col[0], ascending=False)\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def get_weighted_importance(self) -> pd.DataFrame:\n",
    "        \"\"\"Calculate feature importance weighted by model R¬≤.\"\"\"\n",
    "        if self.importance_df is None or self.metrics_df is None:\n",
    "            return None\n",
    "        \n",
    "        # Get R¬≤ for each model (test set)\n",
    "        r2_by_model = {}\n",
    "        for model in self.models.keys():\n",
    "            model_metrics = self.metrics_df[\n",
    "                (self.metrics_df['model_folder'] == model) & \n",
    "                (self.metrics_df['split'] == 'test')\n",
    "            ]\n",
    "            r2_row = model_metrics[model_metrics['metric'].str.contains('R', case=False, na=False)]\n",
    "            if not r2_row.empty:\n",
    "                r2_by_model[model] = max(0, r2_row['value'].iloc[0])  # Clip negative R¬≤\n",
    "        \n",
    "        # Filter importance data - keep only 'gain' type if available, or 'permutation'\n",
    "        if 'type' in self.importance_df.columns:\n",
    "            # Prefer gain, then permutation\n",
    "            if 'gain' in self.importance_df['type'].values:\n",
    "                imp_df = self.importance_df[self.importance_df['type'] == 'gain'].copy()\n",
    "            elif 'permutation' in self.importance_df['type'].values:\n",
    "                imp_df = self.importance_df[self.importance_df['type'] == 'permutation'].copy()\n",
    "            else:\n",
    "                imp_df = self.importance_df.copy()\n",
    "        else:\n",
    "            imp_df = self.importance_df.copy()\n",
    "        \n",
    "        # Calculate weighted importance\n",
    "        results = []\n",
    "        features = imp_df['feature'].unique()\n",
    "        \n",
    "        for feature in features:\n",
    "            feat_data = imp_df[imp_df['feature'] == feature]\n",
    "            \n",
    "            # Raw stats\n",
    "            n_models = len(feat_data)\n",
    "            raw_importance = feat_data['importance'].values\n",
    "            \n",
    "            # Weighted by R¬≤\n",
    "            weighted_sum = 0\n",
    "            weight_sum = 0\n",
    "            \n",
    "            for _, row in feat_data.iterrows():\n",
    "                model = row['model_folder']\n",
    "                imp = row['importance']\n",
    "                r2 = r2_by_model.get(model, 0)\n",
    "                \n",
    "                weighted_sum += imp * r2\n",
    "                weight_sum += r2\n",
    "            \n",
    "            weighted_importance = weighted_sum / weight_sum if weight_sum > 0 else 0\n",
    "            \n",
    "            # Rank in each model (1 = most important)\n",
    "            ranks = []\n",
    "            for model in feat_data['model_folder'].unique():\n",
    "                model_imp = imp_df[imp_df['model_folder'] == model].copy()\n",
    "                model_imp['rank'] = model_imp['importance'].rank(ascending=False)\n",
    "                feat_rank = model_imp[model_imp['feature'] == feature]['rank'].values\n",
    "                if len(feat_rank) > 0:\n",
    "                    ranks.append(feat_rank[0])\n",
    "            \n",
    "            avg_rank = np.mean(ranks) if ranks else np.nan\n",
    "            best_rank = min(ranks) if ranks else np.nan\n",
    "            \n",
    "            results.append({\n",
    "                'feature': feature,\n",
    "                'n_models': n_models,\n",
    "                'mean_importance': np.mean(raw_importance),\n",
    "                'std_importance': np.std(raw_importance),\n",
    "                'weighted_importance': weighted_importance,\n",
    "                'avg_rank': avg_rank,\n",
    "                'best_rank': best_rank,\n",
    "                # Composite score: weighted importance * (1 / avg_rank)\n",
    "                'composite_score': weighted_importance * (1 / avg_rank) if avg_rank > 0 else 0\n",
    "            })\n",
    "        \n",
    "        result_df = pd.DataFrame(results)\n",
    "        result_df = result_df.sort_values('weighted_importance', ascending=False)\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def get_importance_matrix(self) -> pd.DataFrame:\n",
    "        \"\"\"Create a matrix of feature importance by model.\"\"\"\n",
    "        if self.importance_df is None:\n",
    "            return None\n",
    "        \n",
    "        # Filter for gain type if available\n",
    "        if 'type' in self.importance_df.columns:\n",
    "            if 'gain' in self.importance_df['type'].values:\n",
    "                imp_df = self.importance_df[self.importance_df['type'] == 'gain']\n",
    "            else:\n",
    "                imp_df = self.importance_df[self.importance_df['type'] == self.importance_df['type'].iloc[0]]\n",
    "        else:\n",
    "            imp_df = self.importance_df\n",
    "        \n",
    "        # Pivot\n",
    "        matrix = imp_df.pivot_table(\n",
    "            index='feature',\n",
    "            columns='model_folder',\n",
    "            values='importance',\n",
    "            aggfunc='first'\n",
    "        )\n",
    "        \n",
    "        return matrix\n",
    "\n",
    "\n",
    "print(\"MLResultsScanner defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5: Visualization Functions\n",
    "\n",
    "def plot_model_comparison(comparison_df: pd.DataFrame) -> go.Figure:\n",
    "    \"\"\"Create model comparison bar charts.\"\"\"\n",
    "    # Identify metric columns\n",
    "    metric_cols = [c for c in comparison_df.columns if c != 'model_folder']\n",
    "    n_metrics = len(metric_cols)\n",
    "    \n",
    "    # Determine which metrics are \"higher is better\" vs \"lower is better\"\n",
    "    higher_better = ['R¬≤', 'R2', 'Explained Var', 'Explained_Var']\n",
    "    lower_better = ['RMSE', 'MAE', 'MSE']\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=n_metrics,\n",
    "        subplot_titles=metric_cols\n",
    "    )\n",
    "    \n",
    "    colors = px.colors.qualitative.Set2\n",
    "    \n",
    "    for i, metric in enumerate(metric_cols, 1):\n",
    "        # Sort appropriately\n",
    "        ascending = any(m in metric for m in lower_better)\n",
    "        sorted_df = comparison_df.sort_values(metric, ascending=ascending)\n",
    "        \n",
    "        # Color by performance\n",
    "        n = len(sorted_df)\n",
    "        if ascending:  # Lower is better\n",
    "            bar_colors = [colors[j % len(colors)] for j in range(n)]\n",
    "        else:  # Higher is better\n",
    "            bar_colors = [colors[(n-1-j) % len(colors)] for j in range(n)]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                y=sorted_df['model_folder'],\n",
    "                x=sorted_df[metric],\n",
    "                orientation='h',\n",
    "                marker_color=bar_colors,\n",
    "                text=[f\"{v:.4f}\" for v in sorted_df[metric]],\n",
    "                textposition='outside',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=i\n",
    "        )\n",
    "        fig.update_xaxes(title_text=metric, row=1, col=i)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=400,\n",
    "        title='Model Performance Comparison (Test Set)',\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_importance_heatmap(matrix_df: pd.DataFrame, r2_by_model: Dict = None) -> go.Figure:\n",
    "    \"\"\"Create heatmap of feature importance across models.\"\"\"\n",
    "    # Sort features by mean importance\n",
    "    matrix_df = matrix_df.copy()\n",
    "    matrix_df['mean'] = matrix_df.mean(axis=1)\n",
    "    matrix_df = matrix_df.sort_values('mean', ascending=True)\n",
    "    matrix_df = matrix_df.drop('mean', axis=1)\n",
    "    \n",
    "    # Sort columns by R¬≤ if provided\n",
    "    if r2_by_model:\n",
    "        cols_sorted = sorted(matrix_df.columns, key=lambda x: r2_by_model.get(x, 0), reverse=True)\n",
    "        matrix_df = matrix_df[cols_sorted]\n",
    "    \n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=matrix_df.values,\n",
    "        x=matrix_df.columns,\n",
    "        y=matrix_df.index,\n",
    "        colorscale='Blues',\n",
    "        text=[[f\"{v:.3f}\" if pd.notna(v) else \"\" for v in row] for row in matrix_df.values],\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 10},\n",
    "        hoverongaps=False\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Feature Importance Across Models',\n",
    "        xaxis_title='Model',\n",
    "        yaxis_title='Feature',\n",
    "        height=max(400, len(matrix_df) * 25)\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_weighted_importance(weighted_df: pd.DataFrame) -> go.Figure:\n",
    "    \"\"\"Create weighted importance visualization.\"\"\"\n",
    "    # Sort by weighted importance\n",
    "    df = weighted_df.sort_values('weighted_importance', ascending=True).copy()\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=(\n",
    "            'Weighted Importance (by R¬≤)',\n",
    "            'Mean Importance (raw)',\n",
    "            'Composite Score'\n",
    "        ),\n",
    "        column_widths=[0.4, 0.3, 0.3]\n",
    "    )\n",
    "    \n",
    "    # Weighted importance\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            y=df['feature'],\n",
    "            x=df['weighted_importance'],\n",
    "            orientation='h',\n",
    "            marker_color='steelblue',\n",
    "            name='Weighted',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Mean importance with error bars\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            y=df['feature'],\n",
    "            x=df['mean_importance'],\n",
    "            orientation='h',\n",
    "            marker_color='coral',\n",
    "            error_x=dict(type='data', array=df['std_importance']),\n",
    "            name='Mean ¬± Std',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Composite score\n",
    "    df_composite = df.sort_values('composite_score', ascending=True)\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            y=df_composite['feature'],\n",
    "            x=df_composite['composite_score'],\n",
    "            orientation='h',\n",
    "            marker_color='purple',\n",
    "            name='Composite',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=3\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=max(450, len(df) * 30),\n",
    "        title='Feature Importance Analysis (weighted by model performance)'\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text='Importance √ó R¬≤', row=1, col=1)\n",
    "    fig.update_xaxes(title_text='Mean Importance', row=1, col=2)\n",
    "    fig.update_xaxes(title_text='Weighted √ó (1/Rank)', row=1, col=3)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_importance_radar(weighted_df: pd.DataFrame, top_n: int = 8) -> go.Figure:\n",
    "    \"\"\"Create radar chart of top features.\"\"\"\n",
    "    # Get top N features\n",
    "    top_features = weighted_df.nlargest(top_n, 'weighted_importance')\n",
    "    \n",
    "    # Normalize metrics to 0-1 scale\n",
    "    metrics = ['weighted_importance', 'mean_importance', 'composite_score']\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for _, row in top_features.iterrows():\n",
    "        values = []\n",
    "        for m in metrics:\n",
    "            max_val = weighted_df[m].max()\n",
    "            values.append(row[m] / max_val if max_val > 0 else 0)\n",
    "        values.append(values[0])  # Close the polygon\n",
    "        \n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "            r=values,\n",
    "            theta=metrics + [metrics[0]],\n",
    "            fill='toself',\n",
    "            name=row['feature'],\n",
    "            opacity=0.6\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(visible=True, range=[0, 1])\n",
    "        ),\n",
    "        showlegend=True,\n",
    "        title=f'Top {top_n} Features - Multi-metric Comparison',\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_r2_vs_importance(scanner: MLResultsScanner) -> go.Figure:\n",
    "    \"\"\"Plot feature importance vs model R¬≤ to show which features matter in good models.\"\"\"\n",
    "    if scanner.importance_df is None or scanner.metrics_df is None:\n",
    "        return None\n",
    "    \n",
    "    # Get R¬≤ for each model\n",
    "    r2_by_model = {}\n",
    "    for model in scanner.models.keys():\n",
    "        model_metrics = scanner.metrics_df[\n",
    "            (scanner.metrics_df['model_folder'] == model) & \n",
    "            (scanner.metrics_df['split'] == 'test')\n",
    "        ]\n",
    "        r2_row = model_metrics[model_metrics['metric'].str.contains('R', case=False, na=False)]\n",
    "        if not r2_row.empty:\n",
    "            r2_by_model[model] = r2_row['value'].iloc[0]\n",
    "    \n",
    "    # Filter importance data\n",
    "    if 'type' in scanner.importance_df.columns:\n",
    "        if 'gain' in scanner.importance_df['type'].values:\n",
    "            imp_df = scanner.importance_df[scanner.importance_df['type'] == 'gain'].copy()\n",
    "        else:\n",
    "            imp_df = scanner.importance_df.copy()\n",
    "    else:\n",
    "        imp_df = scanner.importance_df.copy()\n",
    "    \n",
    "    # Add R¬≤ to importance data\n",
    "    imp_df['model_r2'] = imp_df['model_folder'].map(r2_by_model)\n",
    "    \n",
    "    # Create scatter plot\n",
    "    fig = px.scatter(\n",
    "        imp_df,\n",
    "        x='model_r2',\n",
    "        y='importance',\n",
    "        color='feature',\n",
    "        hover_data=['model_folder'],\n",
    "        title='Feature Importance vs Model R¬≤',\n",
    "        labels={'model_r2': 'Model R¬≤ (Test)', 'importance': 'Feature Importance'}\n",
    "    )\n",
    "    \n",
    "    # Add trend lines for top features\n",
    "    fig.update_traces(marker=dict(size=10, opacity=0.7))\n",
    "    fig.update_layout(height=500)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "print(\"Visualization functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6: Main Dashboard\n",
    "\n",
    "# Initialize\n",
    "scanner = MLResultsScanner()\n",
    "fb = FolderBrowser('.', label='üìÅ Select Results Directory')\n",
    "\n",
    "# Widgets\n",
    "w_scan_btn = widgets.Button(\n",
    "    description='üîç Scan Results',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "w_analyze_btn = widgets.Button(\n",
    "    description='üìä Analyze & Compare',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='100%', height='45px')\n",
    ")\n",
    "w_scan_info = widgets.HTML(\"<i>Select a folder and click 'Scan Results'</i>\")\n",
    "w_log = widgets.Output(layout=widgets.Layout(border='1px solid #ccc', max_height='200px', overflow='auto'))\n",
    "w_plots = widgets.Output(layout=widgets.Layout(border='1px solid #ccc', min_height='400px'))\n",
    "\n",
    "# Event handlers\n",
    "def on_scan(b):\n",
    "    with w_log:\n",
    "        clear_output()\n",
    "        print(f\"Scanning: {fb.path()}\")\n",
    "        \n",
    "        result = scanner.scan(fb.path())\n",
    "        \n",
    "        if result['n_models'] == 0:\n",
    "            w_scan_info.value = \"<span style='color:red'>‚ùå No ML results found!</span>\"\n",
    "            print(\"No model folders with *_metrics_*.csv found\")\n",
    "        else:\n",
    "            w_scan_info.value = f\"\"\"\n",
    "                <div style='background:#e8f6e8; padding:10px; border-radius:5px;'>\n",
    "                    <b>‚úÖ Found {result['n_models']} models:</b><br>\n",
    "                    {', '.join(result['models'])}\n",
    "                </div>\n",
    "            \"\"\"\n",
    "            print(f\"Found {result['n_models']} models\")\n",
    "            for m in result['models']:\n",
    "                print(f\"  üìÅ {m}\")\n",
    "\n",
    "def on_analyze(b):\n",
    "    if not scanner.models:\n",
    "        with w_log:\n",
    "            print(\"‚ùå Scan for results first!\")\n",
    "        return\n",
    "    \n",
    "    with w_log:\n",
    "        clear_output()\n",
    "        print(\"Loading all results...\")\n",
    "        metrics_df, importance_df = scanner.load_all()\n",
    "        print(f\"Loaded metrics: {len(metrics_df) if metrics_df is not None else 0} rows\")\n",
    "        print(f\"Loaded importance: {len(importance_df) if importance_df is not None else 0} rows\")\n",
    "    \n",
    "    with w_plots:\n",
    "        clear_output()\n",
    "        \n",
    "        # 1. Model Comparison\n",
    "        print(\"=\"*60)\n",
    "        print(\"üìä MODEL PERFORMANCE COMPARISON\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        comparison = scanner.get_model_comparison()\n",
    "        if comparison is not None:\n",
    "            display(HTML(comparison.to_html(index=False)))\n",
    "            \n",
    "            fig1 = plot_model_comparison(comparison)\n",
    "            fig1.show()\n",
    "        \n",
    "        # 2. Feature Importance Heatmap\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üî• FEATURE IMPORTANCE HEATMAP\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        matrix = scanner.get_importance_matrix()\n",
    "        if matrix is not None:\n",
    "            # Get R¬≤ by model for sorting\n",
    "            r2_by_model = {}\n",
    "            for model in scanner.models.keys():\n",
    "                model_metrics = scanner.metrics_df[\n",
    "                    (scanner.metrics_df['model_folder'] == model) & \n",
    "                    (scanner.metrics_df['split'] == 'test')\n",
    "                ]\n",
    "                r2_row = model_metrics[model_metrics['metric'].str.contains('R', case=False, na=False)]\n",
    "                if not r2_row.empty:\n",
    "                    r2_by_model[model] = r2_row['value'].iloc[0]\n",
    "            \n",
    "            fig2 = plot_importance_heatmap(matrix, r2_by_model)\n",
    "            fig2.show()\n",
    "        \n",
    "        # 3. Weighted Importance\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üèÜ WEIGHTED FEATURE IMPORTANCE (by Model R¬≤)\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\"\"\\nFeatures are weighted by how well the model performed.\n",
    "A feature that is important in a model with R¬≤=0.9 counts more \n",
    "than the same feature in a model with R¬≤=0.3.\\n\"\"\")\n",
    "        \n",
    "        weighted = scanner.get_weighted_importance()\n",
    "        if weighted is not None:\n",
    "            # Display table\n",
    "            display_cols = ['feature', 'n_models', 'weighted_importance', 'mean_importance', 'avg_rank', 'composite_score']\n",
    "            display_df = weighted[display_cols].copy()\n",
    "            display_df.columns = ['Feature', '# Models', 'Weighted Imp.', 'Mean Imp.', 'Avg Rank', 'Composite Score']\n",
    "            display(HTML(display_df.round(4).to_html(index=False)))\n",
    "            \n",
    "            fig3 = plot_weighted_importance(weighted)\n",
    "            fig3.show()\n",
    "        \n",
    "        # 4. R¬≤ vs Importance scatter\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìà IMPORTANCE vs MODEL PERFORMANCE\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\"\"\\nThis plot shows which features are important in the best-performing models.\n",
    "Points in the upper-right are features that are important in good models.\\n\"\"\")\n",
    "        \n",
    "        fig4 = plot_r2_vs_importance(scanner)\n",
    "        if fig4:\n",
    "            fig4.show()\n",
    "        \n",
    "        # 5. Summary\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìã SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if comparison is not None:\n",
    "            r2_col = [c for c in comparison.columns if 'R' in c][0] if any('R' in c for c in comparison.columns) else None\n",
    "            if r2_col:\n",
    "                best_model = comparison.loc[comparison[r2_col].idxmax()]\n",
    "                print(f\"\\nü•á Best Model: {best_model['model_folder']} (R¬≤ = {best_model[r2_col]:.4f})\")\n",
    "        \n",
    "        if weighted is not None:\n",
    "            top_features = weighted.nlargest(5, 'weighted_importance')\n",
    "            print(f\"\\nüéØ Top 5 Features (weighted by R¬≤):\")\n",
    "            for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
    "                print(f\"   {i}. {row['feature']}: {row['weighted_importance']:.4f}\")\n",
    "\n",
    "# Connect events\n",
    "w_scan_btn.on_click(on_scan)\n",
    "w_analyze_btn.on_click(on_analyze)\n",
    "\n",
    "# Build UI\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"\"\"\n",
    "        <div style=\"background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%); \n",
    "                    padding: 15px; border-radius: 8px; margin-bottom: 15px;\">\n",
    "            <h2 style=\"color: white; margin: 0;\">üèÜ ML Model Comparison Dashboard</h2>\n",
    "            <p style=\"color: #e8e8e8; margin: 5px 0 0 0;\">Compare models & find the most important features</p>\n",
    "        </div>\n",
    "    \"\"\"),\n",
    "    \n",
    "    widgets.HTML(\"<h3>1Ô∏è‚É£ Select Results Directory</h3>\"),\n",
    "    fb.w,\n",
    "    widgets.HBox([w_scan_btn]),\n",
    "    w_scan_info,\n",
    "    \n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    \n",
    "    widgets.HTML(\"<h3>2Ô∏è‚É£ Analyze & Compare</h3>\"),\n",
    "    w_analyze_btn,\n",
    "    \n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    \n",
    "    widgets.HTML(\"<b>Log:</b>\"),\n",
    "    w_log,\n",
    "    \n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    \n",
    "    widgets.HTML(\"<h3>3Ô∏è‚É£ Results</h3>\"),\n",
    "    w_plots\n",
    "])\n",
    "\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "help",
   "metadata": {},
   "source": [
    "---\n",
    "## üìñ Help\n",
    "\n",
    "### Metrics Explained:\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| **Weighted Importance** | Feature importance √ó Model R¬≤. Features important in better models score higher. |\n",
    "| **Mean Importance** | Simple average across all models (raw). |\n",
    "| **Avg Rank** | Average ranking position across models (1 = most important). |\n",
    "| **Composite Score** | `Weighted Importance √ó (1/Avg Rank)`. Rewards features that are consistently top-ranked in good models. |\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "- **Weighted Importance**: The main metric. A feature scores high if:\n",
    "  1. It's important in models that perform well (high R¬≤)\n",
    "  2. The importance value itself is high\n",
    "\n",
    "- **Composite Score**: Rewards **consistency**. A feature that ranks #1-2 in all models scores higher than one that ranks #1 in one model but #8 in others.\n",
    "\n",
    "### How to Use:\n",
    "\n",
    "1. Navigate to the folder containing your ML results subfolders\n",
    "2. Click 'Scan Results' to find all models\n",
    "3. Click 'Analyze & Compare' to generate visualizations\n",
    "\n",
    "### Expected Directory Structure:\n",
    "\n",
    "```\n",
    "results/\n",
    "‚îú‚îÄ‚îÄ RandomForest/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ *_metrics_*.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ *_feature_importance_*.csv\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îú‚îÄ‚îÄ GradientBoosting/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ XGBoost/\n",
    "    ‚îî‚îÄ‚îÄ ...\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
