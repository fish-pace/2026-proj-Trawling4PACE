{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# PACE Data Preprocessor for Fishing Potential Correlation\n",
    "\n",
    "Este notebook pré-processa dados PACE (Plankton, Aerosol, Cloud, ocean Ecosystem) para gerar variáveis derivadas relevantes para predição de potencial pesqueiro, conforme métricas descritas no relatório técnico.\n",
    "\n",
    "## Variáveis derivadas:\n",
    "- **chlor_a**: Clorofila-a [mg m⁻³]\n",
    "- **carbon_phyto**: Carbono Fitoplanctônico C_phyto [mg m⁻³]\n",
    "- **chl_c_ratio**: Razão Chl:C - indicador de taxa de crescimento μ\n",
    "- **bbp_s**: Inclinação espectral η do backscattering - proxy para PSD (tamanho de partículas)\n",
    "- **poc**: Carbono Orgânico Particulado [mg m⁻³]\n",
    "- **Kd_490**: Coeficiente de atenuação difusa em 490nm [m⁻¹] - para calcular Z_eu\n",
    "\n",
    "## Estratégias de preenchimento temporal:\n",
    "1. **strict**: Valor exato do dia - NaN permanece NaN\n",
    "2. **filled**: Busca valor válido em janela ±4 dias (prioridade: dia atual > mais próximo > passado em empate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Instalação de dependências\n",
    "import sys, subprocess\n",
    "pkgs = [\"pandas\", \"numpy\", \"xarray\", \"netCDF4\", \"scipy\"]\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + pkgs + [\"--quiet\"])\n",
    "print(\"Dependências instaladas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(f\"OK - numpy {np.__version__}, pandas {pd.__version__}, xarray {xr.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configuração\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURAÇÃO - EDITAR CONFORME NECESSÁRIO\n",
    "# =============================================================================\n",
    "\n",
    "# Diretório com arquivos PACE brutos (subpastas por produto)\n",
    "PACE_DATA_DIR = Path(\"../../data/pace\")\n",
    "\n",
    "# Diretório de saída para NetCDFs processados\n",
    "OUTPUT_DIR = Path(\"./data/pace_processed\")\n",
    "\n",
    "# Janela temporal para preenchimento de NaN (dias antes/depois)\n",
    "TEMPORAL_WINDOW = 4\n",
    "\n",
    "# Mapeamento de produtos PACE para seus arquivos\n",
    "# Padrão esperado: pace_{product}_{YYYYMMDD}.nc\n",
    "PACE_PRODUCTS = {\n",
    "    'carbon': {'var': 'carbon_phyto', 'output_name': 'carbon_phyto'},\n",
    "    'chl': {'var': 'chlor_a', 'output_name': 'chlor_a'},\n",
    "    'iop': {'var': 'bbp_s', 'output_name': 'bbp_s'},  # η - inclinação espectral\n",
    "    'poc': {'var': 'poc', 'output_name': 'poc'},\n",
    "    'kd': {'var': 'Kd', 'wavelength_idx': None, 'output_name': 'Kd_490'},  # Kd tem dimensão wavelength\n",
    "    'rrs': {'var': 'Rrs', 'wavelength_idx': None, 'output_name': 'Rrs'},  # Para possíveis derivadas futuras\n",
    "}\n",
    "\n",
    "# Comprimentos de onda de interesse para Kd (490nm é o mais comum para Z_eu)\n",
    "KD_TARGET_WAVELENGTH = 490  # nm\n",
    "\n",
    "print(f\"Diretório PACE: {PACE_DATA_DIR}\")\n",
    "print(f\"Diretório de saída: {OUTPUT_DIR}\")\n",
    "print(f\"Janela temporal: ±{TEMPORAL_WINDOW} dias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Funções de descoberta de arquivos\n",
    "\n",
    "def discover_pace_files(data_dir: Path) -> Dict[str, Dict[datetime, Path]]:\n",
    "    \"\"\"\n",
    "    Descobre arquivos PACE organizados por produto e data.\n",
    "    \n",
    "    Espera estrutura:\n",
    "    data_dir/\n",
    "      pace_carbon_20250328.nc\n",
    "      pace_chl_20250328.nc\n",
    "      ...\n",
    "    \n",
    "    Retorna:\n",
    "        Dict[produto -> Dict[data -> arquivo]]\n",
    "    \"\"\"\n",
    "    files_by_product = defaultdict(dict)\n",
    "    \n",
    "    if not data_dir.exists():\n",
    "        print(f\"AVISO: Diretório {data_dir} não existe!\")\n",
    "        return files_by_product\n",
    "    \n",
    "    # Padrão: pace_{produto}_{YYYYMMDD}.nc\n",
    "    pattern = re.compile(r'pace_([a-z]+)_(\\d{8})\\.nc', re.IGNORECASE)\n",
    "    \n",
    "    for f in data_dir.glob('*.nc'):\n",
    "        match = pattern.match(f.name)\n",
    "        if match:\n",
    "            product = match.group(1).lower()\n",
    "            date_str = match.group(2)\n",
    "            try:\n",
    "                date = datetime.strptime(date_str, '%Y%m%d')\n",
    "                files_by_product[product][date] = f\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    # Resumo\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ARQUIVOS PACE DESCOBERTOS\")\n",
    "    print(\"=\"*60)\n",
    "    for product, dates in sorted(files_by_product.items()):\n",
    "        date_range = f\"{min(dates).strftime('%Y-%m-%d')} a {max(dates).strftime('%Y-%m-%d')}\"\n",
    "        print(f\"  {product:12s}: {len(dates):4d} arquivos ({date_range})\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return dict(files_by_product)\n",
    "\n",
    "\n",
    "def get_available_dates(files_by_product: Dict) -> List[datetime]:\n",
    "    \"\"\"Retorna lista de datas que têm pelo menos um produto disponível.\"\"\"\n",
    "    all_dates = set()\n",
    "    for dates in files_by_product.values():\n",
    "        all_dates.update(dates.keys())\n",
    "    return sorted(all_dates)\n",
    "\n",
    "\n",
    "# Testar descoberta\n",
    "pace_files = discover_pace_files(PACE_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Funções de leitura de variáveis PACE\n",
    "\n",
    "def read_pace_variable(filepath: Path, var_name: str, \n",
    "                       wavelength_target: Optional[int] = None) -> Tuple[Optional[np.ndarray], np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Lê uma variável de um arquivo PACE.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Caminho do arquivo NetCDF\n",
    "        var_name: Nome da variável\n",
    "        wavelength_target: Se a variável tem dimensão wavelength, extrai este comprimento de onda\n",
    "    \n",
    "    Returns:\n",
    "        (data_2d, lats, lons) ou (None, None, None) se erro\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with xr.open_dataset(filepath) as ds:\n",
    "            if var_name not in ds.data_vars:\n",
    "                return None, None, None\n",
    "            \n",
    "            data = ds[var_name]\n",
    "            lats = ds['lat'].values\n",
    "            lons = ds['lon'].values\n",
    "            \n",
    "            # Se tem dimensão wavelength, extrair banda específica\n",
    "            if 'wavelength' in data.dims and wavelength_target is not None:\n",
    "                wavelengths = ds['wavelength'].values\n",
    "                # Encontrar índice mais próximo\n",
    "                idx = np.argmin(np.abs(wavelengths - wavelength_target))\n",
    "                actual_wl = wavelengths[idx]\n",
    "                data = data.isel(wavelength=idx)\n",
    "                # print(f\"  → Extraído wavelength={actual_wl}nm (alvo: {wavelength_target}nm)\")\n",
    "            \n",
    "            # Converter para numpy e aplicar scale/offset se necessário\n",
    "            values = data.values.astype(np.float32)\n",
    "            \n",
    "            # Tratar fill values\n",
    "            fill_val = data.attrs.get('_FillValue', -32767)\n",
    "            values = np.where(values == fill_val, np.nan, values)\n",
    "            values = np.where(np.abs(values) > 1e10, np.nan, values)\n",
    "            \n",
    "            return values, lats, lons\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ERRO lendo {filepath}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def get_kd_wavelength_index(filepath: Path, target_wl: int = 490) -> Optional[int]:\n",
    "    \"\"\"Obtém o índice do comprimento de onda mais próximo do alvo para Kd.\"\"\"\n",
    "    try:\n",
    "        with xr.open_dataset(filepath) as ds:\n",
    "            if 'wavelength' in ds.dims:\n",
    "                wls = ds['wavelength'].values\n",
    "                idx = np.argmin(np.abs(wls - target_wl))\n",
    "                return int(idx), int(wls[idx])\n",
    "    except:\n",
    "        pass\n",
    "    return None, None\n",
    "\n",
    "\n",
    "print(\"Funções de leitura definidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Funções de preenchimento temporal\n",
    "\n",
    "def find_nearest_valid_date(target_date: datetime, \n",
    "                            available_dates: Dict[datetime, Path],\n",
    "                            window: int = 4) -> Optional[datetime]:\n",
    "    \"\"\"\n",
    "    Encontra a data válida mais próxima dentro da janela temporal.\n",
    "    \n",
    "    Regra de prioridade:\n",
    "    1. Se target_date tem dados, retorna ela\n",
    "    2. Procura a data mais próxima em [target-window, target+window]\n",
    "    3. Em caso de empate de distância, prefere o passado\n",
    "    \n",
    "    Args:\n",
    "        target_date: Data alvo\n",
    "        available_dates: Dict de datas disponíveis -> arquivos\n",
    "        window: Janela em dias\n",
    "    \n",
    "    Returns:\n",
    "        Data mais próxima ou None se não encontrar\n",
    "    \"\"\"\n",
    "    if target_date in available_dates:\n",
    "        return target_date\n",
    "    \n",
    "    candidates = []\n",
    "    for dt in available_dates:\n",
    "        delta = (dt - target_date).days\n",
    "        if -window <= delta <= window:\n",
    "            candidates.append((abs(delta), delta, dt))\n",
    "    \n",
    "    if not candidates:\n",
    "        return None\n",
    "    \n",
    "    # Ordenar por: distância absoluta, depois por delta (negativo = passado = preferido)\n",
    "    candidates.sort(key=lambda x: (x[0], x[1]))\n",
    "    return candidates[0][2]\n",
    "\n",
    "\n",
    "class PACEDataLoader:\n",
    "    \"\"\"\n",
    "    Carrega dados PACE com cache e preenchimento temporal.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, files_by_product: Dict[str, Dict[datetime, Path]], \n",
    "                 temporal_window: int = 4):\n",
    "        self.files = files_by_product\n",
    "        self.window = temporal_window\n",
    "        self._cache = {}  # (product, date) -> (data, lats, lons)\n",
    "        self._grid_info = None  # (lats, lons) do primeiro arquivo carregado\n",
    "    \n",
    "    def _get_cache_key(self, product: str, date: datetime) -> Tuple:\n",
    "        return (product, date)\n",
    "    \n",
    "    def _load_to_cache(self, product: str, date: datetime) -> bool:\n",
    "        \"\"\"Carrega dados para o cache. Retorna True se sucesso.\"\"\"\n",
    "        key = self._get_cache_key(product, date)\n",
    "        if key in self._cache:\n",
    "            return self._cache[key][0] is not None\n",
    "        \n",
    "        if product not in self.files or date not in self.files[product]:\n",
    "            self._cache[key] = (None, None, None)\n",
    "            return False\n",
    "        \n",
    "        filepath = self.files[product][date]\n",
    "        config = PACE_PRODUCTS.get(product, {})\n",
    "        var_name = config.get('var', product)\n",
    "        \n",
    "        # Para Kd, usar wavelength target\n",
    "        wl_target = KD_TARGET_WAVELENGTH if product == 'kd' else None\n",
    "        \n",
    "        data, lats, lons = read_pace_variable(filepath, var_name, wl_target)\n",
    "        self._cache[key] = (data, lats, lons)\n",
    "        \n",
    "        # Guardar info da grade\n",
    "        if data is not None and self._grid_info is None:\n",
    "            self._grid_info = (lats.copy(), lons.copy())\n",
    "        \n",
    "        return data is not None\n",
    "    \n",
    "    def get_value_strict(self, product: str, date: datetime, \n",
    "                         lat: float, lon: float) -> float:\n",
    "        \"\"\"\n",
    "        Obtém valor interpolado para o dia exato.\n",
    "        Retorna NaN se não houver dado no dia.\n",
    "        \"\"\"\n",
    "        self._load_to_cache(product, date)\n",
    "        data, lats, lons = self._cache.get(self._get_cache_key(product, date), (None, None, None))\n",
    "        \n",
    "        if data is None:\n",
    "            return np.nan\n",
    "        \n",
    "        return self._interpolate(data, lats, lons, lat, lon)\n",
    "    \n",
    "    def get_value_filled(self, product: str, target_date: datetime,\n",
    "                         lat: float, lon: float) -> Tuple[float, Optional[datetime]]:\n",
    "        \"\"\"\n",
    "        Obtém valor interpolado, buscando em janela temporal se necessário.\n",
    "        \n",
    "        Returns:\n",
    "            (valor, data_usada) - data_usada é None se o valor for NaN\n",
    "        \"\"\"\n",
    "        if product not in self.files:\n",
    "            return np.nan, None\n",
    "        \n",
    "        # Gerar lista de datas a tentar, ordenada por prioridade\n",
    "        dates_to_try = self._get_dates_by_priority(target_date, self.files[product])\n",
    "        \n",
    "        for dt in dates_to_try:\n",
    "            self._load_to_cache(product, dt)\n",
    "            data, lats, lons = self._cache.get(self._get_cache_key(product, dt), (None, None, None))\n",
    "            \n",
    "            if data is None:\n",
    "                continue\n",
    "            \n",
    "            val = self._interpolate(data, lats, lons, lat, lon)\n",
    "            if np.isfinite(val):\n",
    "                return val, dt\n",
    "        \n",
    "        return np.nan, None\n",
    "    \n",
    "    def _get_dates_by_priority(self, target: datetime, \n",
    "                               available: Dict[datetime, Path]) -> List[datetime]:\n",
    "        \"\"\"\n",
    "        Retorna lista de datas ordenadas por prioridade.\n",
    "        Prioridade: distância absoluta, depois passado em empate.\n",
    "        \"\"\"\n",
    "        candidates = []\n",
    "        for dt in available:\n",
    "            delta = (dt - target).days\n",
    "            if -self.window <= delta <= self.window:\n",
    "                # Para ordenação: (distância_abs, delta positivo penalizado)\n",
    "                # delta < 0 (passado) -> ordenar antes\n",
    "                candidates.append((abs(delta), 0 if delta <= 0 else 1, dt))\n",
    "        \n",
    "        candidates.sort()\n",
    "        return [c[2] for c in candidates]\n",
    "    \n",
    "    def _interpolate(self, data: np.ndarray, lats: np.ndarray, \n",
    "                     lons: np.ndarray, lat: float, lon: float) -> float:\n",
    "        \"\"\"Interpola bilinearmente o valor na posição lat/lon.\"\"\"\n",
    "        try:\n",
    "            # Verificar bounds\n",
    "            if lat < lats.min() or lat > lats.max():\n",
    "                return np.nan\n",
    "            if lon < lons.min() or lon > lons.max():\n",
    "                return np.nan\n",
    "            \n",
    "            # Garantir ordem monotônica\n",
    "            if lats[0] > lats[-1]:\n",
    "                lats = lats[::-1]\n",
    "                data = data[::-1, :]\n",
    "            if lons[0] > lons[-1]:\n",
    "                lons = lons[::-1]\n",
    "                data = data[:, ::-1]\n",
    "            \n",
    "            interp = RegularGridInterpolator(\n",
    "                (lats, lons), data, \n",
    "                method='linear',\n",
    "                bounds_error=False, \n",
    "                fill_value=np.nan\n",
    "            )\n",
    "            return float(interp([[lat, lon]])[0])\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Limpa o cache para liberar memória.\"\"\"\n",
    "        self._cache.clear()\n",
    "\n",
    "\n",
    "print(\"Classes de carregamento definidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Função principal de extração\n",
    "\n",
    "def extract_pace_variables(csv_path: str,\n",
    "                           pace_files: Dict[str, Dict[datetime, Path]],\n",
    "                           lat_col: str = 'LATITUDE_DEC',\n",
    "                           lon_col: str = 'LONGITUDE_DEC', \n",
    "                           date_col: str = 'GMT_DATE_TIME',\n",
    "                           temporal_window: int = 4,\n",
    "                           products: List[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrai variáveis PACE para cada ponto do CSV.\n",
    "    \n",
    "    Para cada produto, gera duas colunas:\n",
    "    - {produto}_strict: valor do dia exato (NaN se não disponível)\n",
    "    - {produto}_filled: valor preenchido da janela temporal\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Caminho do CSV com dados de pesca\n",
    "        pace_files: Dicionário de arquivos PACE por produto/data\n",
    "        lat_col, lon_col, date_col: Nomes das colunas\n",
    "        temporal_window: Janela em dias para preenchimento\n",
    "        products: Lista de produtos a extrair (None = todos disponíveis)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame com variáveis extraídas\n",
    "    \"\"\"\n",
    "    # Carregar CSV\n",
    "    print(f\"\\nCarregando CSV: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"  → {len(df)} registros\")\n",
    "    \n",
    "    # Preparar colunas de coordenadas\n",
    "    df['_lat'] = pd.to_numeric(df[lat_col], errors='coerce')\n",
    "    df['_lon'] = pd.to_numeric(df[lon_col], errors='coerce')\n",
    "    df['_date'] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "    \n",
    "    # Filtrar registros válidos\n",
    "    valid_mask = df['_lat'].notna() & df['_lon'].notna() & df['_date'].notna()\n",
    "    df = df[valid_mask].reset_index(drop=True)\n",
    "    print(f\"  → {len(df)} registros com coordenadas válidas\")\n",
    "    \n",
    "    # Produtos a processar\n",
    "    if products is None:\n",
    "        products = list(pace_files.keys())\n",
    "    products = [p for p in products if p in pace_files]\n",
    "    print(f\"  → Produtos a extrair: {products}\")\n",
    "    \n",
    "    # Inicializar loader\n",
    "    loader = PACEDataLoader(pace_files, temporal_window)\n",
    "    \n",
    "    # Criar colunas de resultado\n",
    "    n_rows = len(df)\n",
    "    results = {}\n",
    "    for product in products:\n",
    "        output_name = PACE_PRODUCTS.get(product, {}).get('output_name', product)\n",
    "        results[f'{output_name}_strict'] = np.full(n_rows, np.nan)\n",
    "        results[f'{output_name}_filled'] = np.full(n_rows, np.nan)\n",
    "        results[f'{output_name}_filled_date_offset'] = np.full(n_rows, np.nan)  # Dias de diferença\n",
    "    \n",
    "    # Extrair valores\n",
    "    print(\"\\nExtraindo valores PACE...\")\n",
    "    for product in products:\n",
    "        output_name = PACE_PRODUCTS.get(product, {}).get('output_name', product)\n",
    "        print(f\"  Processando {product} → {output_name}...\")\n",
    "        \n",
    "        for i in range(n_rows):\n",
    "            if i % 1000 == 0 and i > 0:\n",
    "                print(f\"    {i}/{n_rows}...\")\n",
    "            \n",
    "            lat = df.loc[i, '_lat']\n",
    "            lon = df.loc[i, '_lon']\n",
    "            date = df.loc[i, '_date'].to_pydatetime()\n",
    "            \n",
    "            # Valor strict (dia exato)\n",
    "            val_strict = loader.get_value_strict(product, date, lat, lon)\n",
    "            results[f'{output_name}_strict'][i] = val_strict\n",
    "            \n",
    "            # Valor filled (com janela temporal)\n",
    "            val_filled, date_used = loader.get_value_filled(product, date, lat, lon)\n",
    "            results[f'{output_name}_filled'][i] = val_filled\n",
    "            \n",
    "            if date_used is not None:\n",
    "                offset = (date_used - date).days\n",
    "                results[f'{output_name}_filled_date_offset'][i] = offset\n",
    "        \n",
    "        # Estatísticas\n",
    "        n_strict = np.sum(np.isfinite(results[f'{output_name}_strict']))\n",
    "        n_filled = np.sum(np.isfinite(results[f'{output_name}_filled']))\n",
    "        print(f\"    → strict: {n_strict}/{n_rows} ({100*n_strict/n_rows:.1f}%)\")\n",
    "        print(f\"    → filled: {n_filled}/{n_rows} ({100*n_filled/n_rows:.1f}%)\")\n",
    "        \n",
    "        # Limpar cache periódicamente para economizar memória\n",
    "        loader.clear_cache()\n",
    "    \n",
    "    # Adicionar colunas ao DataFrame\n",
    "    for col, values in results.items():\n",
    "        df[col] = values\n",
    "    \n",
    "    # Calcular variáveis derivadas\n",
    "    print(\"\\nCalculando variáveis derivadas...\")\n",
    "    \n",
    "    # Razão Chl:C (indicador de taxa de crescimento μ)\n",
    "    if 'chlor_a_strict' in df.columns and 'carbon_phyto_strict' in df.columns:\n",
    "        df['chl_c_ratio_strict'] = df['chlor_a_strict'] / df['carbon_phyto_strict']\n",
    "        df['chl_c_ratio_filled'] = df['chlor_a_filled'] / df['carbon_phyto_filled']\n",
    "        print(\"  → chl_c_ratio calculado\")\n",
    "    \n",
    "    # Limpar colunas temporárias\n",
    "    df = df.drop(columns=['_lat', '_lon', '_date'], errors='ignore')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Função de extração definida.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Função para criar NetCDF consolidado (opcional)\n",
    "\n",
    "def create_pace_daily_composite(pace_files: Dict[str, Dict[datetime, Path]],\n",
    "                                 output_dir: Path,\n",
    "                                 products: List[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Cria arquivos NetCDF diários consolidados com todas as variáveis PACE.\n",
    "    \n",
    "    Isso facilita o uso no correlation_dashboard, que espera uma estrutura:\n",
    "    output_dir/pace_composite_YYYYMMDD.nc\n",
    "    \n",
    "    Cada arquivo contém:\n",
    "    - chlor_a\n",
    "    - carbon_phyto\n",
    "    - bbp_s\n",
    "    - poc\n",
    "    - Kd_490\n",
    "    - chl_c_ratio (derivada)\n",
    "    \"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if products is None:\n",
    "        products = ['carbon', 'chl', 'iop', 'poc', 'kd']\n",
    "    \n",
    "    # Descobrir todas as datas disponíveis\n",
    "    all_dates = set()\n",
    "    for product in products:\n",
    "        if product in pace_files:\n",
    "            all_dates.update(pace_files[product].keys())\n",
    "    all_dates = sorted(all_dates)\n",
    "    \n",
    "    print(f\"\\nCriando composites para {len(all_dates)} datas...\")\n",
    "    \n",
    "    for date in all_dates:\n",
    "        print(f\"  {date.strftime('%Y-%m-%d')}...\", end=\" \")\n",
    "        \n",
    "        data_arrays = {}\n",
    "        lats, lons = None, None\n",
    "        \n",
    "        for product in products:\n",
    "            if product not in pace_files or date not in pace_files[product]:\n",
    "                continue\n",
    "            \n",
    "            filepath = pace_files[product][date]\n",
    "            config = PACE_PRODUCTS.get(product, {})\n",
    "            var_name = config.get('var', product)\n",
    "            output_name = config.get('output_name', product)\n",
    "            \n",
    "            wl_target = KD_TARGET_WAVELENGTH if product == 'kd' else None\n",
    "            data, lat_arr, lon_arr = read_pace_variable(filepath, var_name, wl_target)\n",
    "            \n",
    "            if data is not None:\n",
    "                data_arrays[output_name] = data\n",
    "                if lats is None:\n",
    "                    lats, lons = lat_arr, lon_arr\n",
    "        \n",
    "        if not data_arrays or lats is None:\n",
    "            print(\"SKIP (sem dados)\")\n",
    "            continue\n",
    "        \n",
    "        # Calcular derivadas\n",
    "        if 'chlor_a' in data_arrays and 'carbon_phyto' in data_arrays:\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                chl_c = data_arrays['chlor_a'] / data_arrays['carbon_phyto']\n",
    "                chl_c = np.where(np.isfinite(chl_c), chl_c, np.nan)\n",
    "            data_arrays['chl_c_ratio'] = chl_c\n",
    "        \n",
    "        # Criar Dataset xarray\n",
    "        ds = xr.Dataset(\n",
    "            {name: (['lat', 'lon'], arr) for name, arr in data_arrays.items()},\n",
    "            coords={'lat': lats, 'lon': lons}\n",
    "        )\n",
    "        \n",
    "        # Adicionar atributos\n",
    "        ds.attrs['title'] = 'PACE OCI Daily Composite for Fishing Potential Analysis'\n",
    "        ds.attrs['date'] = date.strftime('%Y-%m-%d')\n",
    "        ds.attrs['source'] = 'NASA PACE OCI L3 products'\n",
    "        ds.attrs['processing'] = 'Consolidated by pace_preprocessor.ipynb'\n",
    "        \n",
    "        # Atributos das variáveis\n",
    "        var_attrs = {\n",
    "            'chlor_a': {'long_name': 'Chlorophyll-a concentration', 'units': 'mg m^-3'},\n",
    "            'carbon_phyto': {'long_name': 'Phytoplankton Carbon', 'units': 'mg m^-3'},\n",
    "            'bbp_s': {'long_name': 'Backscattering spectral slope (eta)', 'units': 'dimensionless'},\n",
    "            'poc': {'long_name': 'Particulate Organic Carbon', 'units': 'mg m^-3'},\n",
    "            'Kd_490': {'long_name': 'Diffuse attenuation coefficient at 490nm', 'units': 'm^-1'},\n",
    "            'chl_c_ratio': {'long_name': 'Chlorophyll:Carbon ratio (growth rate proxy)', 'units': 'mg Chl / mg C'},\n",
    "        }\n",
    "        for var in ds.data_vars:\n",
    "            if var in var_attrs:\n",
    "                ds[var].attrs.update(var_attrs[var])\n",
    "        \n",
    "        # Salvar\n",
    "        outfile = output_dir / f\"pace_composite_{date.strftime('%Y%m%d')}.nc\"\n",
    "        ds.to_netcdf(outfile)\n",
    "        print(f\"OK ({len(data_arrays)} vars)\")\n",
    "    \n",
    "    print(f\"\\nComposites salvos em: {output_dir}\")\n",
    "\n",
    "\n",
    "print(\"Função de criação de composites definida.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage",
   "metadata": {},
   "source": [
    "---\n",
    "## Uso\n",
    "\n",
    "### Opção 1: Extrair variáveis PACE para um CSV existente\n",
    "Use quando quiser adicionar variáveis PACE diretamente a um arquivo de dados de pesca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: OPÇÃO 1 - Extrair para CSV\n",
    "# Descomente e ajuste conforme necessário\n",
    "\n",
    "# CSV_INPUT = \"./data/fishing_data.csv\"  # Seu arquivo de dados\n",
    "# CSV_OUTPUT = \"./data/fishing_data_with_pace.csv\"\n",
    "\n",
    "# # Extrair\n",
    "# df_result = extract_pace_variables(\n",
    "#     csv_path=CSV_INPUT,\n",
    "#     pace_files=pace_files,\n",
    "#     lat_col='LATITUDE_DEC',    # Ajustar conforme seu CSV\n",
    "#     lon_col='LONGITUDE_DEC',   # Ajustar conforme seu CSV\n",
    "#     date_col='GMT_DATE_TIME',  # Ajustar conforme seu CSV\n",
    "#     temporal_window=4,\n",
    "#     products=['carbon', 'chl', 'iop', 'poc', 'kd']  # Produtos desejados\n",
    "# )\n",
    "\n",
    "# # Salvar\n",
    "# df_result.to_csv(CSV_OUTPUT, index=False)\n",
    "# print(f\"\\nSalvo: {CSV_OUTPUT}\")\n",
    "# print(f\"Colunas adicionadas: {[c for c in df_result.columns if 'strict' in c or 'filled' in c]}\")\n",
    "\n",
    "print(\"Opção 1 (extração para CSV) - descomente e configure acima para executar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "option2",
   "metadata": {},
   "source": [
    "### Opção 2: Criar NetCDFs consolidados\n",
    "Use quando quiser que o correlation_dashboard leia os dados PACE de uma pasta organizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10",
   "metadata": {},
   "outputs": [],
   "source": [
    " Cell 10: OPÇÃO 2 - Criar composites NetCDF\n",
    " Descomente para executar\n",
    "\n",
    " create_pace_daily_composite(\n",
    "     pace_files=pace_files,\n",
    "     output_dir=OUTPUT_DIR,\n",
    "     products=['carbon', 'chl', 'iop', 'poc', 'kd']\n",
    " )\n",
    "\n",
    "print(\"Opção 2 (criar composites) - descomente acima para executar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "option3",
   "metadata": {},
   "source": [
    "### Opção 3: Uso interativo / diagnóstico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Diagnóstico - verificar cobertura de dados\n",
    "\n",
    "def diagnose_pace_coverage(pace_files: Dict[str, Dict[datetime, Path]],\n",
    "                            test_dates: List[datetime] = None):\n",
    "    \"\"\"\n",
    "    Mostra diagnóstico da cobertura de dados PACE.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DIAGNÓSTICO DE COBERTURA PACE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    all_dates = get_available_dates(pace_files)\n",
    "    if not all_dates:\n",
    "        print(\"Nenhum arquivo PACE encontrado!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nPeríodo coberto: {all_dates[0].strftime('%Y-%m-%d')} a {all_dates[-1].strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Total de datas únicas: {len(all_dates)}\")\n",
    "    \n",
    "    # Cobertura por produto\n",
    "    print(\"\\nCobertura por produto:\")\n",
    "    for product, dates in sorted(pace_files.items()):\n",
    "        coverage = len(dates) / len(all_dates) * 100 if all_dates else 0\n",
    "        print(f\"  {product:12s}: {len(dates):4d} dias ({coverage:5.1f}%)\")\n",
    "    \n",
    "    # Testar algumas datas\n",
    "    if test_dates:\n",
    "        print(\"\\nTeste de disponibilidade:\")\n",
    "        for dt in test_dates:\n",
    "            print(f\"  {dt.strftime('%Y-%m-%d')}:\")\n",
    "            for product in pace_files:\n",
    "                status = \"✓\" if dt in pace_files[product] else \"✗\"\n",
    "                print(f\"    {product}: {status}\")\n",
    "\n",
    "# Executar diagnóstico\n",
    "if pace_files:\n",
    "    diagnose_pace_coverage(pace_files)\n",
    "else:\n",
    "    print(\"Nenhum arquivo PACE encontrado. Verifique o diretório configurado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Teste de extração pontual\n",
    "\n",
    "def test_extraction(pace_files: Dict, \n",
    "                    test_lat: float, test_lon: float, \n",
    "                    test_date: datetime,\n",
    "                    window: int = 4):\n",
    "    \"\"\"\n",
    "    Testa extração de valores para um ponto específico.\n",
    "    \"\"\"\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"TESTE DE EXTRAÇÃO\")\n",
    "    print(f\"=\"*60)\n",
    "    print(f\"Coordenadas: ({test_lat}, {test_lon})\")\n",
    "    print(f\"Data: {test_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Janela temporal: ±{window} dias\")\n",
    "    print()\n",
    "    \n",
    "    loader = PACEDataLoader(pace_files, window)\n",
    "    \n",
    "    for product in pace_files:\n",
    "        output_name = PACE_PRODUCTS.get(product, {}).get('output_name', product)\n",
    "        \n",
    "        val_strict = loader.get_value_strict(product, test_date, test_lat, test_lon)\n",
    "        val_filled, date_used = loader.get_value_filled(product, test_date, test_lat, test_lon)\n",
    "        \n",
    "        print(f\"{output_name}:\")\n",
    "        print(f\"  strict: {val_strict:.4f}\" if np.isfinite(val_strict) else \"  strict: NaN\")\n",
    "        if np.isfinite(val_filled):\n",
    "            offset = (date_used - test_date).days if date_used else 0\n",
    "            print(f\"  filled: {val_filled:.4f} (offset: {offset:+d} dias)\")\n",
    "        else:\n",
    "            print(f\"  filled: NaN\")\n",
    "        print()\n",
    "\n",
    "# Exemplo de teste (descomente e ajuste)\n",
    "# test_extraction(\n",
    "#     pace_files,\n",
    "#     test_lat=-23.0,  # Exemplo: próximo a Arraial do Cabo\n",
    "#     test_lon=-42.0,\n",
    "#     test_date=datetime(2025, 3, 28),\n",
    "#     window=4\n",
    "# )\n",
    "\n",
    "print(\"Função de teste definida. Descomente a chamada acima para testar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Resumo das variáveis geradas\n",
    "\n",
    "| Variável | Descrição | Relevância para Pesca |\n",
    "|----------|-----------|----------------------|\n",
    "| `chlor_a` | Clorofila-a [mg m⁻³] | Indicador tradicional de biomassa |\n",
    "| `carbon_phyto` | C_phyto [mg m⁻³] | Biomassa real de carbono |\n",
    "| `chl_c_ratio` | Chl:C | Proxy de taxa de crescimento μ (alto=crescimento ativo) |\n",
    "| `bbp_s` | η (eta) | Inclinação PSD: baixo→partículas grandes→cadeia curta eficiente |\n",
    "| `poc` | POC [mg m⁻³] | Carbono particulado total (inclui detritos) |\n",
    "| `Kd_490` | Kd em 490nm [m⁻¹] | Atenuação → Z_eu = 4.6/Kd (profundidade eufótica) |\n",
    "\n",
    "### Sufixos:\n",
    "- `_strict`: Valor do dia exato (NaN se não disponível)\n",
    "- `_filled`: Valor preenchido da janela ±4 dias\n",
    "- `_filled_date_offset`: Dias de diferença do valor preenchido"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
