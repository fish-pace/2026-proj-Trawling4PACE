{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Install dependencies\n",
    "import sys, subprocess\n",
    "pkgs = [\"pandas\", \"numpy\", \"plotly\", \"ipywidgets\", \"kaleido\"]\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + pkgs + [\"--quiet\"])\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä ML Model Comparison Dashboard\n",
      "Compare models from the same split method\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Imports\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä ML Model Comparison Dashboard\")\n",
    "print(\"Compare models from the same split method\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder browser loaded.\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: Folder Browser Widget\n",
    "\n",
    "class FolderBrowser:\n",
    "    \"\"\"Interactive folder browser widget.\"\"\"\n",
    "    \n",
    "    def __init__(self, start='.', label='Folder'):\n",
    "        self.cur = Path(start).resolve()\n",
    "        self.sel = self.cur\n",
    "        self.label = label\n",
    "        \n",
    "        self.html = widgets.HTML(f\"<code>{self.cur}</code>\")\n",
    "        self.dd = widgets.Select(\n",
    "            options=self._list(), \n",
    "            layout=widgets.Layout(width='100%', height='150px')\n",
    "        )\n",
    "        self.b_up = widgets.Button(description='‚Üë Up', layout=widgets.Layout(width='70px'))\n",
    "        self.b_in = widgets.Button(description='‚Üí Enter', layout=widgets.Layout(width='80px'))\n",
    "        self.b_sel = widgets.Button(description='‚úì Select', button_style='success', \n",
    "                                    layout=widgets.Layout(width='80px'))\n",
    "        self.selhtml = widgets.HTML(f\"<b>Selected:</b> <code>{self.sel}</code>\")\n",
    "        \n",
    "        self.b_up.on_click(lambda b: self._up())\n",
    "        self.b_in.on_click(lambda b: self._enter())\n",
    "        self.b_sel.on_click(lambda b: self._select())\n",
    "        \n",
    "        self.w = widgets.VBox([\n",
    "            widgets.HTML(f\"<b>{label}</b>\"),\n",
    "            self.html, self.dd,\n",
    "            widgets.HBox([self.b_up, self.b_in, self.b_sel]),\n",
    "            self.selhtml\n",
    "        ])\n",
    "    \n",
    "    def _list(self):\n",
    "        try:\n",
    "            items = ['.']\n",
    "            for x in sorted(self.cur.iterdir()):\n",
    "                if x.is_dir() and not x.name.startswith('.'):\n",
    "                    items.append(f\"üìÅ {x.name}\")\n",
    "            return items\n",
    "        except:\n",
    "            return ['.']\n",
    "    \n",
    "    def _refresh(self):\n",
    "        self.html.value = f\"<code>{self.cur}</code>\"\n",
    "        self.dd.options = self._list()\n",
    "    \n",
    "    def _up(self):\n",
    "        if self.cur.parent != self.cur:\n",
    "            self.cur = self.cur.parent\n",
    "            self._refresh()\n",
    "    \n",
    "    def _enter(self):\n",
    "        if self.dd.value and self.dd.value.startswith('üìÅ'):\n",
    "            name = self.dd.value[2:].strip()\n",
    "            p = self.cur / name\n",
    "            if p.is_dir():\n",
    "                self.cur = p\n",
    "                self._refresh()\n",
    "    \n",
    "    def _select(self):\n",
    "        self.sel = self.cur\n",
    "        self.selhtml.value = f\"<b>Selected:</b> <code>{self.sel}</code>\"\n",
    "    \n",
    "    def path(self):\n",
    "        return self.sel\n",
    "\n",
    "\n",
    "print(\"Folder browser loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loader functions defined.\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: Model Results Loader\n",
    "\n",
    "def load_model_results(method_folder):\n",
    "    \"\"\"\n",
    "    Load all model results from a method folder.\n",
    "    \n",
    "    Expected structure:\n",
    "    method_folder/\n",
    "        ‚îú‚îÄ‚îÄ ModelName1/\n",
    "        ‚îÇ   ‚îú‚îÄ‚îÄ *_metrics.csv\n",
    "        ‚îÇ   ‚îú‚îÄ‚îÄ *_predictions.csv\n",
    "        ‚îÇ   ‚îú‚îÄ‚îÄ *_feature_importance.csv\n",
    "        ‚îÇ   ‚îî‚îÄ‚îÄ *_summary.txt\n",
    "        ‚îú‚îÄ‚îÄ ModelName2/\n",
    "        ‚îî‚îÄ‚îÄ ...\n",
    "    \"\"\"\n",
    "    method_folder = Path(method_folder)\n",
    "    results = {}\n",
    "    \n",
    "    # Detect method name from folder\n",
    "    method_name = method_folder.name\n",
    "    \n",
    "    # Find all model subfolders\n",
    "    model_folders = [f for f in method_folder.iterdir() if f.is_dir()]\n",
    "    \n",
    "    print(f\"\\nüìÇ Loading results from: {method_folder}\")\n",
    "    print(f\"   Method: {method_name}\")\n",
    "    print(f\"   Found {len(model_folders)} model folders\")\n",
    "    \n",
    "    for model_folder in sorted(model_folders):\n",
    "        model_name = model_folder.name\n",
    "        model_data = {\n",
    "            'name': model_name,\n",
    "            'folder': model_folder,\n",
    "            'metrics': None,\n",
    "            'predictions': None,\n",
    "            'feature_importance': None,\n",
    "        }\n",
    "        \n",
    "        # Find and load metrics CSV\n",
    "        metrics_files = list(model_folder.glob('*_metrics.csv'))\n",
    "        if metrics_files:\n",
    "            model_data['metrics'] = pd.read_csv(metrics_files[0])\n",
    "        \n",
    "        # Find and load predictions CSV\n",
    "        pred_files = list(model_folder.glob('*_predictions.csv'))\n",
    "        if pred_files:\n",
    "            model_data['predictions'] = pd.read_csv(pred_files[0])\n",
    "        \n",
    "        # Find and load feature importance CSV\n",
    "        fi_files = list(model_folder.glob('*_feature_importance.csv'))\n",
    "        if fi_files:\n",
    "            model_data['feature_importance'] = pd.read_csv(fi_files[0])\n",
    "        \n",
    "        # Check if we have at least metrics\n",
    "        if model_data['metrics'] is not None:\n",
    "            results[model_name] = model_data\n",
    "            print(f\"   ‚úì {model_name}: metrics loaded\")\n",
    "        else:\n",
    "            print(f\"   ‚ö† {model_name}: no metrics found, skipping\")\n",
    "    \n",
    "    return results, method_name\n",
    "\n",
    "\n",
    "def build_comparison_dataframe(results):\n",
    "    \"\"\"\n",
    "    Build a unified DataFrame for comparison.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for model_name, data in results.items():\n",
    "        metrics_df = data['metrics']\n",
    "        \n",
    "        # Extract metrics for train and test\n",
    "        train_metrics = metrics_df[metrics_df['split'] == 'train'].set_index('metric')['value'].to_dict()\n",
    "        test_metrics = metrics_df[metrics_df['split'] == 'test'].set_index('metric')['value'].to_dict()\n",
    "        \n",
    "        row = {\n",
    "            'Model': model_name,\n",
    "            'R¬≤_train': train_metrics.get('R¬≤', np.nan),\n",
    "            'R¬≤_test': test_metrics.get('R¬≤', np.nan),\n",
    "            'RMSE_train': train_metrics.get('RMSE', np.nan),\n",
    "            'RMSE_test': test_metrics.get('RMSE', np.nan),\n",
    "            'MAE_train': train_metrics.get('MAE', np.nan),\n",
    "            'MAE_test': test_metrics.get('MAE', np.nan),\n",
    "            'ExpVar_train': train_metrics.get('Explained Var', np.nan),\n",
    "            'ExpVar_test': test_metrics.get('Explained Var', np.nan),\n",
    "        }\n",
    "        \n",
    "        # Calculate gaps (overfitting indicators)\n",
    "        row['R¬≤_gap'] = row['R¬≤_train'] - row['R¬≤_test']\n",
    "        row['RMSE_gap'] = row['RMSE_test'] - row['RMSE_train']\n",
    "        row['MAE_gap'] = row['MAE_test'] - row['MAE_train']\n",
    "        \n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Sort by test R¬≤ (best first)\n",
    "    df = df.sort_values('R¬≤_test', ascending=False).reset_index(drop=True)\n",
    "    df['Rank'] = df.index + 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Model loader functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization functions defined.\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: Visualization Functions\n",
    "\n",
    "def create_r2_comparison_figure(df, method_name):\n",
    "    \"\"\"\n",
    "    Create R¬≤ comparison bar chart (train vs test for each model).\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Sort by test R¬≤\n",
    "    df_sorted = df.sort_values('R¬≤_test', ascending=True)\n",
    "    \n",
    "    # Train bars\n",
    "    fig.add_trace(go.Bar(\n",
    "        y=df_sorted['Model'],\n",
    "        x=df_sorted['R¬≤_train'],\n",
    "        name='Train R¬≤',\n",
    "        orientation='h',\n",
    "        marker_color='#3498db',\n",
    "        text=[f\"{v:.4f}\" for v in df_sorted['R¬≤_train']],\n",
    "        textposition='inside',\n",
    "    ))\n",
    "    \n",
    "    # Test bars\n",
    "    fig.add_trace(go.Bar(\n",
    "        y=df_sorted['Model'],\n",
    "        x=df_sorted['R¬≤_test'],\n",
    "        name='Test R¬≤',\n",
    "        orientation='h',\n",
    "        marker_color='#e74c3c',\n",
    "        text=[f\"{v:.4f}\" for v in df_sorted['R¬≤_test']],\n",
    "        textposition='inside',\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"<b>R¬≤ Comparison</b><br><sup>Method: {method_name} | Higher is better</sup>\",\n",
    "        xaxis_title='R¬≤ Score',\n",
    "        yaxis_title='Model',\n",
    "        barmode='group',\n",
    "        height=max(400, len(df) * 50),\n",
    "        legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='center', x=0.5),\n",
    "        xaxis=dict(range=[0, 1.05])\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_error_comparison_figure(df, method_name):\n",
    "    \"\"\"\n",
    "    Create RMSE and MAE comparison.\n",
    "    \"\"\"\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('RMSE (lower is better)', 'MAE (lower is better)'),\n",
    "        horizontal_spacing=0.15\n",
    "    )\n",
    "    \n",
    "    # Sort by test RMSE\n",
    "    df_sorted = df.sort_values('RMSE_test', ascending=False)\n",
    "    \n",
    "    # RMSE\n",
    "    fig.add_trace(go.Bar(\n",
    "        y=df_sorted['Model'],\n",
    "        x=df_sorted['RMSE_train'],\n",
    "        name='Train RMSE',\n",
    "        orientation='h',\n",
    "        marker_color='#3498db',\n",
    "        showlegend=True,\n",
    "    ), row=1, col=1)\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        y=df_sorted['Model'],\n",
    "        x=df_sorted['RMSE_test'],\n",
    "        name='Test RMSE',\n",
    "        orientation='h',\n",
    "        marker_color='#e74c3c',\n",
    "        showlegend=True,\n",
    "    ), row=1, col=1)\n",
    "    \n",
    "    # MAE\n",
    "    fig.add_trace(go.Bar(\n",
    "        y=df_sorted['Model'],\n",
    "        x=df_sorted['MAE_train'],\n",
    "        name='Train MAE',\n",
    "        orientation='h',\n",
    "        marker_color='#2ecc71',\n",
    "        showlegend=True,\n",
    "    ), row=1, col=2)\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        y=df_sorted['Model'],\n",
    "        x=df_sorted['MAE_test'],\n",
    "        name='Test MAE',\n",
    "        orientation='h',\n",
    "        marker_color='#e67e22',\n",
    "        showlegend=True,\n",
    "    ), row=1, col=2)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"<b>Error Metrics Comparison</b><br><sup>Method: {method_name}</sup>\",\n",
    "        barmode='group',\n",
    "        height=max(400, len(df) * 50),\n",
    "        legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='center', x=0.5),\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_overfitting_figure(df, method_name):\n",
    "    \"\"\"\n",
    "    Create overfitting analysis figure (train-test gap).\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Sort by R¬≤ gap (most overfitting first)\n",
    "    df_sorted = df.sort_values('R¬≤_gap', ascending=True)\n",
    "    \n",
    "    # Color based on overfitting level\n",
    "    colors = []\n",
    "    for gap in df_sorted['R¬≤_gap']:\n",
    "        if gap > 0.15:\n",
    "            colors.append('#e74c3c')  # Red - high overfitting\n",
    "        elif gap > 0.05:\n",
    "            colors.append('#f39c12')  # Orange - moderate\n",
    "        else:\n",
    "            colors.append('#27ae60')  # Green - good\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        y=df_sorted['Model'],\n",
    "        x=df_sorted['R¬≤_gap'],\n",
    "        orientation='h',\n",
    "        marker_color=colors,\n",
    "        text=[f\"{v:.4f}\" for v in df_sorted['R¬≤_gap']],\n",
    "        textposition='outside',\n",
    "    ))\n",
    "    \n",
    "    # Add threshold lines\n",
    "    fig.add_vline(x=0.05, line_dash='dash', line_color='orange', \n",
    "                  annotation_text='Moderate', annotation_position='top')\n",
    "    fig.add_vline(x=0.15, line_dash='dash', line_color='red',\n",
    "                  annotation_text='High', annotation_position='top')\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"<b>Overfitting Analysis (R¬≤ Gap: Train - Test)</b><br><sup>Method: {method_name} | Lower is better (green < 0.05 < orange < 0.15 < red)</sup>\",\n",
    "        xaxis_title='R¬≤ Gap (Train - Test)',\n",
    "        yaxis_title='Model',\n",
    "        height=max(400, len(df) * 50),\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_radar_comparison_figure(df, method_name):\n",
    "    \"\"\"\n",
    "    Create radar chart comparing models on multiple metrics.\n",
    "    \"\"\"\n",
    "    # Normalize metrics for radar (0-1 scale, higher is better)\n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    # R¬≤ already 0-1, higher is better\n",
    "    df_norm['R¬≤_norm'] = df_norm['R¬≤_test']\n",
    "    \n",
    "    # RMSE: invert (lower is better -> higher is better)\n",
    "    rmse_max = df_norm['RMSE_test'].max()\n",
    "    df_norm['RMSE_norm'] = 1 - (df_norm['RMSE_test'] / rmse_max) if rmse_max > 0 else 0\n",
    "    \n",
    "    # MAE: invert\n",
    "    mae_max = df_norm['MAE_test'].max()\n",
    "    df_norm['MAE_norm'] = 1 - (df_norm['MAE_test'] / mae_max) if mae_max > 0 else 0\n",
    "    \n",
    "    # Generalization: 1 - gap (lower gap is better)\n",
    "    gap_max = df_norm['R¬≤_gap'].max()\n",
    "    df_norm['Gen_norm'] = 1 - (df_norm['R¬≤_gap'] / gap_max) if gap_max > 0 else 1\n",
    "    \n",
    "    categories = ['R¬≤ (test)', 'Low RMSE', 'Low MAE', 'Generalization']\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    colors = px.colors.qualitative.Set2\n",
    "    \n",
    "    for i, (_, row) in enumerate(df_norm.iterrows()):\n",
    "        values = [row['R¬≤_norm'], row['RMSE_norm'], row['MAE_norm'], row['Gen_norm']]\n",
    "        values.append(values[0])  # Close the radar\n",
    "        \n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "            r=values,\n",
    "            theta=categories + [categories[0]],\n",
    "            fill='toself',\n",
    "            name=row['Model'],\n",
    "            line_color=colors[i % len(colors)],\n",
    "            opacity=0.6,\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(visible=True, range=[0, 1])\n",
    "        ),\n",
    "        title=f\"<b>Multi-Metric Radar Comparison</b><br><sup>Method: {method_name} | Larger area = better model</sup>\",\n",
    "        height=600,\n",
    "        showlegend=True,\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_predictions_scatter(results, method_name):\n",
    "    \"\"\"\n",
    "    Create combined scatter plot of predictions for all models.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    colors = px.colors.qualitative.Set2\n",
    "    \n",
    "    all_obs = []\n",
    "    all_pred = []\n",
    "    \n",
    "    for i, (model_name, data) in enumerate(results.items()):\n",
    "        if data['predictions'] is not None:\n",
    "            pred_df = data['predictions']\n",
    "            test_data = pred_df[pred_df['split'] == 'test']\n",
    "            \n",
    "            if len(test_data) > 0:\n",
    "                obs = test_data['observed'].values\n",
    "                pred = test_data['predicted'].values\n",
    "                \n",
    "                all_obs.extend(obs)\n",
    "                all_pred.extend(pred)\n",
    "                \n",
    "                # Subsample if too many points\n",
    "                if len(obs) > 500:\n",
    "                    idx = np.random.choice(len(obs), 500, replace=False)\n",
    "                    obs = obs[idx]\n",
    "                    pred = pred[idx]\n",
    "                \n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=obs,\n",
    "                    y=pred,\n",
    "                    mode='markers',\n",
    "                    name=model_name,\n",
    "                    marker=dict(\n",
    "                        color=colors[i % len(colors)],\n",
    "                        size=6,\n",
    "                        opacity=0.5\n",
    "                    ),\n",
    "                    hovertemplate=f\"{model_name}<br>Obs: %{{x:.2f}}<br>Pred: %{{y:.2f}}<extra></extra>\"\n",
    "                ))\n",
    "    \n",
    "    # Add 1:1 line\n",
    "    if all_obs:\n",
    "        min_val = min(min(all_obs), min(all_pred))\n",
    "        max_val = max(max(all_obs), max(all_pred))\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[min_val, max_val],\n",
    "            y=[min_val, max_val],\n",
    "            mode='lines',\n",
    "            name='1:1 line',\n",
    "            line=dict(color='black', dash='dash', width=2)\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"<b>Test Predictions: All Models</b><br><sup>Method: {method_name}</sup>\",\n",
    "        xaxis_title='Observed',\n",
    "        yaxis_title='Predicted',\n",
    "        height=600,\n",
    "        legend=dict(orientation='v', yanchor='top', y=0.99, xanchor='left', x=1.02),\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_feature_importance_heatmap(results, method_name):\n",
    "    \"\"\"\n",
    "    Create heatmap of feature importance across models.\n",
    "    \"\"\"\n",
    "    # Collect all feature importances\n",
    "    importance_data = {}\n",
    "    all_features = set()\n",
    "    \n",
    "    for model_name, data in results.items():\n",
    "        if data['feature_importance'] is not None:\n",
    "            fi_df = data['feature_importance']\n",
    "            # Get gain importance (most common)\n",
    "            gain_fi = fi_df[fi_df['type'] == 'gain']\n",
    "            if len(gain_fi) > 0:\n",
    "                importance_data[model_name] = dict(zip(gain_fi['feature'], gain_fi['importance']))\n",
    "                all_features.update(gain_fi['feature'].tolist())\n",
    "    \n",
    "    if not importance_data:\n",
    "        return None\n",
    "    \n",
    "    # Build matrix\n",
    "    all_features = sorted(all_features)\n",
    "    models = list(importance_data.keys())\n",
    "    \n",
    "    matrix = []\n",
    "    for feature in all_features:\n",
    "        row = []\n",
    "        for model in models:\n",
    "            row.append(importance_data[model].get(feature, 0))\n",
    "        matrix.append(row)\n",
    "    \n",
    "    # Normalize per model (column)\n",
    "    matrix = np.array(matrix)\n",
    "    col_max = matrix.max(axis=0, keepdims=True)\n",
    "    col_max[col_max == 0] = 1\n",
    "    matrix_norm = matrix / col_max\n",
    "    \n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=matrix_norm,\n",
    "        x=models,\n",
    "        y=all_features,\n",
    "        colorscale='Blues',\n",
    "        text=[[f\"{v:.4f}\" for v in row] for row in matrix],\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 8},\n",
    "        hovertemplate=\"Feature: %{y}<br>Model: %{x}<br>Importance: %{text}<extra></extra>\"\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"<b>Feature Importance Heatmap (Gain)</b><br><sup>Method: {method_name} | Normalized per model</sup>\",\n",
    "        xaxis_title='Model',\n",
    "        yaxis_title='Feature',\n",
    "        height=max(400, len(all_features) * 25),\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_ranking_table_figure(df, method_name):\n",
    "    \"\"\"\n",
    "    Create a visual ranking table.\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    df_display = df[['Rank', 'Model', 'R¬≤_test', 'R¬≤_train', 'R¬≤_gap', 'RMSE_test', 'MAE_test']].copy()\n",
    "    \n",
    "    # Format values\n",
    "    df_display['R¬≤_test'] = df_display['R¬≤_test'].apply(lambda x: f\"{x:.4f}\")\n",
    "    df_display['R¬≤_train'] = df_display['R¬≤_train'].apply(lambda x: f\"{x:.4f}\")\n",
    "    df_display['R¬≤_gap'] = df_display['R¬≤_gap'].apply(lambda x: f\"{x:.4f}\")\n",
    "    df_display['RMSE_test'] = df_display['RMSE_test'].apply(lambda x: f\"{x:.4f}\")\n",
    "    df_display['MAE_test'] = df_display['MAE_test'].apply(lambda x: f\"{x:.4f}\")\n",
    "    \n",
    "    # Color rows based on rank\n",
    "    colors = []\n",
    "    for rank in df_display['Rank']:\n",
    "        if rank == 1:\n",
    "            colors.append('#d4edda')  # Green - best\n",
    "        elif rank == 2:\n",
    "            colors.append('#fff3cd')  # Yellow - 2nd\n",
    "        elif rank == 3:\n",
    "            colors.append('#ffeeba')  # Light yellow - 3rd\n",
    "        else:\n",
    "            colors.append('#ffffff')  # White\n",
    "    \n",
    "    fig = go.Figure(data=[go.Table(\n",
    "        header=dict(\n",
    "            values=['<b>Rank</b>', '<b>Model</b>', '<b>R¬≤ Test</b>', '<b>R¬≤ Train</b>', \n",
    "                    '<b>R¬≤ Gap</b>', '<b>RMSE Test</b>', '<b>MAE Test</b>'],\n",
    "            fill_color='#2c3e50',\n",
    "            font=dict(color='white', size=12),\n",
    "            align='center',\n",
    "            height=35\n",
    "        ),\n",
    "        cells=dict(\n",
    "            values=[df_display[col] for col in df_display.columns],\n",
    "            fill_color=[colors],\n",
    "            align='center',\n",
    "            font=dict(size=11),\n",
    "            height=30\n",
    "        )\n",
    "    )])\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f\"<b>üèÜ Model Ranking</b><br><sup>Method: {method_name} | Sorted by Test R¬≤</sup>\",\n",
    "        height=max(300, len(df) * 35 + 100)\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "print(\"Visualization functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison Dashboard class defined.\n"
     ]
    }
   ],
   "source": [
    "# CELL 6: Main Comparison Dashboard\n",
    "\n",
    "class ModelComparisonDashboard:\n",
    "    def __init__(self):\n",
    "        self.results = None\n",
    "        self.method_name = None\n",
    "        self.comparison_df = None\n",
    "        self.figures = {}\n",
    "        \n",
    "        self._build_ui()\n",
    "    \n",
    "    def _build_ui(self):\n",
    "        style = {'description_width': 'initial'}\n",
    "        \n",
    "        # Folder browser\n",
    "        self.folder_browser = FolderBrowser('.', label='üìÅ Select Method Folder (max_time_dist, random, or overfit)')\n",
    "        \n",
    "        # Load button\n",
    "        self.load_btn = widgets.Button(\n",
    "            description='üìÇ Load Results',\n",
    "            button_style='success',\n",
    "            layout=widgets.Layout(width='200px', height='40px')\n",
    "        )\n",
    "        self.load_btn.on_click(self._load_results)\n",
    "        \n",
    "        # Info display\n",
    "        self.info_html = widgets.HTML(\"<i>Select a method folder and click 'Load Results'</i>\")\n",
    "        \n",
    "        # Output directory for saving\n",
    "        self.save_browser = FolderBrowser('.', label='üíæ Save Figures To')\n",
    "        self.save_btn = widgets.Button(\n",
    "            description='üíæ Save All Figures',\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(width='200px', height='40px')\n",
    "        )\n",
    "        self.save_btn.on_click(self._save_figures)\n",
    "        \n",
    "        # Outputs\n",
    "        self.log_output = widgets.Output(\n",
    "            layout=widgets.Layout(border='1px solid #ccc', max_height='200px', overflow='auto')\n",
    "        )\n",
    "        self.plot_output = widgets.Output(\n",
    "            layout=widgets.Layout(border='1px solid #ccc', min_height='600px')\n",
    "        )\n",
    "        \n",
    "        # Build UI\n",
    "        self.ui = widgets.VBox([\n",
    "            widgets.HTML(\"\"\"\n",
    "                <div style=\"background: linear-gradient(135deg, #2c3e50 0%, #27ae60 100%); \n",
    "                            padding: 15px; border-radius: 8px; margin-bottom: 15px;\">\n",
    "                    <h2 style=\"color: white; margin: 0;\">üìä ML Model Comparison Dashboard</h2>\n",
    "                    <p style=\"color: #ecf0f1; margin: 5px 0 0 0;\">Compare models from the same split method</p>\n",
    "                </div>\n",
    "            \"\"\"),\n",
    "            widgets.HTML('''\n",
    "                <div style=\"background:#fff3cd; padding:10px; border-radius:5px; margin:10px 0;\">\n",
    "                <b>üìã Instructions:</b><br>\n",
    "                <small>\n",
    "                1. Navigate to a method folder (e.g., <code>max_time_dist/</code>, <code>random/</code>, or <code>overfit/</code>)<br>\n",
    "                2. Click \"Load Results\" to read all model subfolders<br>\n",
    "                3. View comparison visualizations<br>\n",
    "                4. Optionally save figures to a folder<br>\n",
    "                <b>Note:</b> Compare models within the SAME method only (not across methods)\n",
    "                </small>\n",
    "                </div>\n",
    "            '''),\n",
    "            self.folder_browser.w,\n",
    "            self.load_btn,\n",
    "            self.info_html,\n",
    "            widgets.HTML('<hr>'),\n",
    "            widgets.HTML('<b>Log:</b>'),\n",
    "            self.log_output,\n",
    "            widgets.HTML('<hr>'),\n",
    "            widgets.HBox([self.save_browser.w, widgets.VBox([widgets.HTML('<br>'), self.save_btn])]),\n",
    "            widgets.HTML('<hr>'),\n",
    "            widgets.HTML('<b>üìà Comparison Results:</b>'),\n",
    "            self.plot_output\n",
    "        ])\n",
    "    \n",
    "    def _load_results(self, b):\n",
    "        \"\"\"Load results from the selected folder.\"\"\"\n",
    "        with self.log_output:\n",
    "            clear_output()\n",
    "            \n",
    "            folder = self.folder_browser.path()\n",
    "            print(f\"Loading from: {folder}\")\n",
    "            \n",
    "            try:\n",
    "                self.results, self.method_name = load_model_results(folder)\n",
    "                \n",
    "                if not self.results:\n",
    "                    self.info_html.value = \"<span style='color:red'>‚ùå No valid model results found!</span>\"\n",
    "                    return\n",
    "                \n",
    "                # Build comparison DataFrame\n",
    "                self.comparison_df = build_comparison_dataframe(self.results)\n",
    "                \n",
    "                self.info_html.value = f\"\"\"\n",
    "                    <div style=\"background:#d4edda; padding:10px; border-radius:5px;\">\n",
    "                        <b>‚úÖ Loaded {len(self.results)} models</b><br>\n",
    "                        Method: <code>{self.method_name}</code><br>\n",
    "                        Models: {', '.join(self.results.keys())}\n",
    "                    </div>\n",
    "                \"\"\"\n",
    "                \n",
    "                # Generate visualizations\n",
    "                self._generate_visualizations()\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.info_html.value = f\"<span style='color:red'>‚ùå Error: {e}</span>\"\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    def _generate_visualizations(self):\n",
    "        \"\"\"Generate all comparison visualizations.\"\"\"\n",
    "        with self.plot_output:\n",
    "            clear_output()\n",
    "            \n",
    "            df = self.comparison_df\n",
    "            method = self.method_name\n",
    "            \n",
    "            print(\"Generating visualizations...\")\n",
    "            \n",
    "            # 1. Ranking Table\n",
    "            print(\"  üìä Creating ranking table...\")\n",
    "            fig_ranking = create_ranking_table_figure(df, method)\n",
    "            fig_ranking.show()\n",
    "            self.figures['ranking'] = fig_ranking\n",
    "            \n",
    "            # 2. R¬≤ Comparison\n",
    "            print(\"  üìä Creating R¬≤ comparison...\")\n",
    "            fig_r2 = create_r2_comparison_figure(df, method)\n",
    "            fig_r2.show()\n",
    "            self.figures['r2_comparison'] = fig_r2\n",
    "            \n",
    "            # 3. Error Comparison\n",
    "            print(\"  üìä Creating error comparison...\")\n",
    "            fig_error = create_error_comparison_figure(df, method)\n",
    "            fig_error.show()\n",
    "            self.figures['error_comparison'] = fig_error\n",
    "            \n",
    "            # 4. Overfitting Analysis\n",
    "            print(\"  üìä Creating overfitting analysis...\")\n",
    "            fig_overfit = create_overfitting_figure(df, method)\n",
    "            fig_overfit.show()\n",
    "            self.figures['overfitting'] = fig_overfit\n",
    "            \n",
    "            # 5. Radar Comparison\n",
    "            print(\"  üìä Creating radar comparison...\")\n",
    "            fig_radar = create_radar_comparison_figure(df, method)\n",
    "            fig_radar.show()\n",
    "            self.figures['radar'] = fig_radar\n",
    "            \n",
    "            # 6. Predictions Scatter\n",
    "            print(\"  üìä Creating predictions scatter...\")\n",
    "            fig_scatter = create_predictions_scatter(self.results, method)\n",
    "            fig_scatter.show()\n",
    "            self.figures['predictions'] = fig_scatter\n",
    "            \n",
    "            # 7. Feature Importance Heatmap\n",
    "            print(\"  üìä Creating feature importance heatmap...\")\n",
    "            fig_heatmap = create_feature_importance_heatmap(self.results, method)\n",
    "            if fig_heatmap:\n",
    "                fig_heatmap.show()\n",
    "                self.figures['importance_heatmap'] = fig_heatmap\n",
    "            else:\n",
    "                print(\"    ‚ö† No feature importance data available\")\n",
    "            \n",
    "            print(\"\\n‚úÖ All visualizations generated!\")\n",
    "            \n",
    "            # Print summary\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"üìã SUMMARY\")\n",
    "            print(\"=\"*60)\n",
    "            best_model = df.iloc[0]\n",
    "            print(f\"üèÜ Best Model: {best_model['Model']}\")\n",
    "            print(f\"   R¬≤ (test): {best_model['R¬≤_test']:.4f}\")\n",
    "            print(f\"   R¬≤ gap: {best_model['R¬≤_gap']:.4f}\")\n",
    "            print(f\"   RMSE (test): {best_model['RMSE_test']:.4f}\")\n",
    "    \n",
    "    def _save_figures(self, b):\n",
    "        \"\"\"Save all figures to the selected folder.\"\"\"\n",
    "        if not self.figures:\n",
    "            with self.log_output:\n",
    "                print(\"‚ùå No figures to save! Load results first.\")\n",
    "            return\n",
    "        \n",
    "        save_dir = self.save_browser.path()\n",
    "        \n",
    "        with self.log_output:\n",
    "            print(f\"\\nüíæ Saving figures to: {save_dir}\")\n",
    "            \n",
    "            for name, fig in self.figures.items():\n",
    "                try:\n",
    "                    filename = f\"comparison_{self.method_name}_{name}.png\"\n",
    "                    filepath = save_dir / filename\n",
    "                    fig.write_image(str(filepath), scale=2)\n",
    "                    print(f\"   ‚úì Saved: {filename}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö† Failed to save {name}: {e}\")\n",
    "            \n",
    "            # Also save comparison CSV\n",
    "            try:\n",
    "                csv_file = save_dir / f\"comparison_{self.method_name}_summary.csv\"\n",
    "                self.comparison_df.to_csv(csv_file, index=False)\n",
    "                print(f\"   ‚úì Saved: {csv_file.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö† Failed to save CSV: {e}\")\n",
    "            \n",
    "            print(\"\\n‚úÖ All files saved!\")\n",
    "    \n",
    "    def show(self):\n",
    "        display(self.ui)\n",
    "\n",
    "\n",
    "print(\"Model Comparison Dashboard class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä ML Model Comparison Dashboard\n",
      "======================================================================\n",
      "\n",
      "Expected folder structure:\n",
      "  {method}/              ‚Üê Select this folder\n",
      "    ‚îú‚îÄ‚îÄ ModelName1/\n",
      "    ‚îÇ   ‚îú‚îÄ‚îÄ *_metrics.csv\n",
      "    ‚îÇ   ‚îú‚îÄ‚îÄ *_predictions.csv\n",
      "    ‚îÇ   ‚îî‚îÄ‚îÄ *_feature_importance.csv\n",
      "    ‚îú‚îÄ‚îÄ ModelName2/\n",
      "    ‚îî‚îÄ‚îÄ ...\n",
      "\n",
      "Visualizations generated:\n",
      "  üèÜ Ranking Table - sorted by test R¬≤\n",
      "  üìä R¬≤ Comparison - train vs test bars\n",
      "  üìâ Error Comparison - RMSE and MAE\n",
      "  ‚ö†Ô∏è Overfitting Analysis - R¬≤ gap\n",
      "  üéØ Radar Chart - multi-metric comparison\n",
      "  üìà Predictions Scatter - all models overlay\n",
      "  üî• Feature Importance Heatmap\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf694bd6ab6048b7bee8bd840210e3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n                <div style=\"background: linear-gradient(135deg, #2c3e50 0%, #27ae‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL 7: Run the dashboard\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä ML Model Comparison Dashboard\")\n",
    "print(\"=\"*70)\n",
    "print(\"\")\n",
    "print(\"Expected folder structure:\")\n",
    "print(\"  {method}/              ‚Üê Select this folder\")\n",
    "print(\"    ‚îú‚îÄ‚îÄ ModelName1/\")\n",
    "print(\"    ‚îÇ   ‚îú‚îÄ‚îÄ *_metrics.csv\")\n",
    "print(\"    ‚îÇ   ‚îú‚îÄ‚îÄ *_predictions.csv\")\n",
    "print(\"    ‚îÇ   ‚îî‚îÄ‚îÄ *_feature_importance.csv\")\n",
    "print(\"    ‚îú‚îÄ‚îÄ ModelName2/\")\n",
    "print(\"    ‚îî‚îÄ‚îÄ ...\")\n",
    "print(\"\")\n",
    "print(\"Visualizations generated:\")\n",
    "print(\"  üèÜ Ranking Table - sorted by test R¬≤\")\n",
    "print(\"  üìä R¬≤ Comparison - train vs test bars\")\n",
    "print(\"  üìâ Error Comparison - RMSE and MAE\")\n",
    "print(\"  ‚ö†Ô∏è Overfitting Analysis - R¬≤ gap\")\n",
    "print(\"  üéØ Radar Chart - multi-metric comparison\")\n",
    "print(\"  üìà Predictions Scatter - all models overlay\")\n",
    "print(\"  üî• Feature Importance Heatmap\")\n",
    "print(\"\")\n",
    "\n",
    "dashboard = ModelComparisonDashboard()\n",
    "dashboard.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69854d1c-793d-48cc-b060-3742a65c614b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
