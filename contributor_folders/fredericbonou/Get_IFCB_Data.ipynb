{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get IFCB Data\n",
    "\n",
    "This is from a notebook written by Gulce Kurtay during OceanHackWeek 2024. [notebook](https://github.com/oceanhackweek/ohw24_proj_pace_us/blob/main/final_notebooks/Hypercoast(PACE)_with_IFCB.ipynb)\n",
    "\n",
    "Woods Hole Dashboard, https://ifcb-data.whoi.edu/dashboard <br>\n",
    "\n",
    "This notebook designed to get in-situ flow cytometry group distribution data.<br>\n",
    "Classified IFCB data is limited, so make sure to search the database then find a group distribution that matches with your interest. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 1  download the phytoplankton group distribution and metadata from the IFCB dashboard, it only downloads the autoclass files not the images<br>\n",
    "STEP 2.1 organize the csv files in group distribution format aligns with lat and long info<br>\n",
    "STEP 2.2 summarize the information into daily format to match up with PACE data <br>\n",
    "STEP 3   Download the PACE data with hypercoast<br>\n",
    "STEP 4   interactive map with spatial chl-a and insitu group distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1.1 \n",
    "#Functions that needed for the downloading the csv files\n",
    "#STEP-1 Download the csv files and lat and long\n",
    "import os\n",
    "import requests\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def collect_bin_ids(start_bin_id, end_bin_id, base_url, dataset, instrument, prefix):\n",
    "    \"\"\"\n",
    "    Collects all bin IDs starting with a given prefix and stops at the end bin ID.\n",
    "\n",
    "    :param start_bin_id: The starting bin ID to begin the search.\n",
    "    :param end_bin_id: The bin ID at which to stop collecting.\n",
    "    :param base_url: The base URL of the API.\n",
    "    :param dataset: The dataset name to filter.\n",
    "    :param instrument: The instrument name to filter.\n",
    "    :param prefix: The prefix to match bin IDs against (e.g., \"D2024\").\n",
    "    :return: A list of matching bin IDs.\n",
    "    \"\"\"\n",
    "    bin_ids = []\n",
    "    current_bin_id = start_bin_id\n",
    "    \n",
    "    while True:\n",
    "        url = f\"{base_url}/api/bin/{current_bin_id}\"\n",
    "        params = {\n",
    "            \"dataset\": dataset,\n",
    "            \"instrument\": instrument,\n",
    "        }\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve data for bin: {current_bin_id}, Status Code: {response.status_code}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        if current_bin_id.startswith(prefix):\n",
    "            bin_ids.append(current_bin_id)\n",
    "            print(f\"Collected bin ID: {current_bin_id}\")\n",
    "        \n",
    "        # Check if we've reached the end bin ID\n",
    "        if current_bin_id == end_bin_id:\n",
    "            print(f\"Reached the end bin ID: {end_bin_id}\")\n",
    "            break\n",
    "        \n",
    "        next_bin_id = data.get('next_bin_id')\n",
    "        if not next_bin_id or not next_bin_id.startswith(prefix):\n",
    "            break\n",
    "        \n",
    "        current_bin_id = next_bin_id\n",
    "    \n",
    "    print(f\"Total bin IDs collected: {len(bin_ids)}\")\n",
    "    return bin_ids\n",
    "\n",
    "def download_file(file_url, output_dir):\n",
    "    \"\"\"\n",
    "    Downloads a specific file from the given URL.\n",
    "\n",
    "    :param file_url: The full URL to the file.\n",
    "    :param output_dir: The directory where the file will be saved.\n",
    "    \"\"\"\n",
    "    file_name = os.path.basename(file_url)\n",
    "    output_file = os.path.join(output_dir, file_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(file_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        with open(output_file, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded: {output_file}\")\n",
    "        return True\n",
    "\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        if response.status_code == 404:\n",
    "            print(f\"File not found (404): {file_url}\")\n",
    "        else:\n",
    "            print(f\"HTTP error occurred: {err} - URL: {file_url}\")\n",
    "        return False\n",
    "    except requests.exceptions.ConnectionError as err:\n",
    "        print(f\"Connection error occurred: {err}\")\n",
    "        return False\n",
    "    except requests.exceptions.Timeout as err:\n",
    "        print(f\"Timeout error occurred: {err}\")\n",
    "        return False\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"An error occurred: {err}\")\n",
    "        return False\n",
    "\n",
    "def fetch_lat_lon(bin_id, base_url, dataset):\n",
    "    \"\"\"\n",
    "    Fetches latitude and longitude for a specific bin ID.\n",
    "\n",
    "    :param bin_id: The bin ID to fetch lat/lon for.\n",
    "    :param base_url: The base URL where the dataset is located.\n",
    "    :param dataset: The dataset name.\n",
    "    :return: A dictionary with latitude and longitude.\n",
    "    \"\"\"\n",
    "    url = f\"{base_url}/api/bin/{bin_id}\"\n",
    "    params = {\n",
    "        \"dataset\": dataset,\n",
    "    }\n",
    "    response = requests.get(url, params=params, timeout=10)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return {\n",
    "            \"bin_id\": bin_id,\n",
    "            \"latitude\": data.get('lat'),\n",
    "            \"longitude\": data.get('lng')\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Failed to fetch lat/lon for bin: {bin_id}, Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def download_autoclass_csvs_and_lat_lon(start_bin_id, end_bin_id, base_url, dataset, instrument, prefix, output_dir, max_workers=5):\n",
    "    \"\"\"\n",
    "    Collects bin IDs, downloads all _class_scores.csv files, and fetches latitude and longitude for each bin.\n",
    "\n",
    "    :param start_bin_id: The starting bin ID to begin the search.\n",
    "    :param end_bin_id: The bin ID at which to stop collecting.\n",
    "    :param base_url: The base URL where the dataset is located.\n",
    "    :param dataset: The dataset name to filter.\n",
    "    :param instrument: The instrument name to filter.\n",
    "    :param prefix: The prefix to match bin IDs against (e.g., \"D2024\").\n",
    "    :param output_dir: The directory where the files will be saved.\n",
    "    :param max_workers: The maximum number of parallel downloads.\n",
    "    \"\"\"\n",
    "    bin_ids = collect_bin_ids(start_bin_id, end_bin_id, base_url, dataset, instrument, prefix)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        lat_lon_results = []\n",
    "        \n",
    "        for bin_id in bin_ids:\n",
    "            # Download the _class_scores.csv file\n",
    "            csv_file_url = f\"{base_url}/{dataset}/{bin_id}_class_scores.csv\"\n",
    "            futures.append(executor.submit(download_file, csv_file_url, output_dir))\n",
    "            \n",
    "            # Fetch latitude and longitude\n",
    "            lat_lon_results.append(fetch_lat_lon(bin_id, base_url, dataset))\n",
    "        \n",
    "        # Wait for all downloads to complete\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "        \n",
    "        # Extract date from start_bin_id for the CSV filename\n",
    "        date_str = start_bin_id.split('T')[0][1:]  # Extracts \"DYYYYMMDD\" and removes the \"D\"\n",
    "        lat_lon_file = os.path.join(output_dir, f\"{date_str}.csv\")\n",
    "        \n",
    "        # Write lat/lon to CSV\n",
    "        if lat_lon_results:\n",
    "            with open(lat_lon_file, 'w', newline='') as csvfile:\n",
    "                fieldnames = ['bin_id', 'latitude', 'longitude']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                \n",
    "                writer.writeheader()\n",
    "                for result in lat_lon_results:\n",
    "                    if result:\n",
    "                        writer.writerow(result)\n",
    "            print(f\"Latitude and Longitude data saved to {lat_lon_file}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected bin ID: D20240215T150055_IFCB010\n",
      "Collected bin ID: D20240215T152410_IFCB010\n",
      "Collected bin ID: D20240215T154723_IFCB010\n",
      "Collected bin ID: D20240215T161037_IFCB010\n",
      "Collected bin ID: D20240215T163352_IFCB010\n",
      "Collected bin ID: D20240215T165704_IFCB010\n",
      "Collected bin ID: D20240215T172017_IFCB010\n",
      "Collected bin ID: D20240215T174329_IFCB010\n",
      "Collected bin ID: D20240215T180642_IFCB010\n",
      "Collected bin ID: D20240215T182954_IFCB010\n",
      "Collected bin ID: D20240215T185306_IFCB010\n",
      "Collected bin ID: D20240215T191619_IFCB010\n",
      "Reached the end bin ID: D20240215T191619_IFCB010\n",
      "Total bin IDs collected: 12\n",
      "Downloaded: ../../data/mvco/D20240215T152410_IFCB010_class_scores.csv\n",
      "Downloaded: ../../data/mvco/D20240215T154723_IFCB010_class_scores.csv\n",
      "Downloaded: ../../data/mvco/D20240215T161037_IFCB010_class_scores.csv\n",
      "Downloaded: ../../data/mvco/D20240215T163352_IFCB010_class_scores.csv\n",
      "Downloaded: ../../data/mvco/D20240215T165704_IFCB010_class_scores.csv\n",
      "Downloaded: ../../data/mvco/D20240215T150055_IFCB010_class_scores.csv\n",
      "Downloaded: ../../data/mvco/D20240215T172017_IFCB010_class_scores.csv\n",
      "Downloaded: ../../data/mvco/D20240215T174329_IFCB010_class_scores.csv\n",
      "Downloaded: ../../data/mvco/D20240215T180642_IFCB010_class_scores.csv\n",
      "Downloaded: ../../data/mvco/D20240215T182954_IFCB010_class_scores.csv\n",
      "Downloaded: ../../data/mvco/D20240215T185306_IFCB010_class_scores.csv\n",
      "Downloaded: ../../data/mvco/D20240215T191619_IFCB010_class_scores.csv\n",
      "Latitude and Longitude data saved to ../../data/mvco/20240215.csv\n"
     ]
    }
   ],
   "source": [
    "#STEP 1.2 application\n",
    "#To get the community composition information from the dashboard, you will need to know specific information about the dataset:\n",
    "#I have added stat and end bin ids because it will take too much time, so you can limit it \n",
    "# Example usage\n",
    "base_url = \"https://ifcb-data.whoi.edu\"\n",
    "dataset = \"mvco\" #Dataset name: Name should be as it spelled in the url \n",
    "instrument = \"IFCB10\" #Instrument number\n",
    "# Roll over the timeline and then you can copy the bin number\n",
    "start_bin_id = \"D20240215T150055_IFCB010\"  # Start with a known valid bin ID \n",
    "end_bin_id = \"D20240215T191619_IFCB010\"  # End at this bin ID\n",
    "#end_bin_id = \"D20241227T181716_IFCB010\"  # End at this bin ID\n",
    "prefix = \"D2024\"  # Prefix to match bin IDs\n",
    "output_dir = r\"../../data/mvco\"  # Directory to save the files\n",
    "\n",
    "# Download all _class_scores.csv files and fetch lat/lon data, stopping at end_bin_id\n",
    "download_autoclass_csvs_and_lat_lon(start_bin_id, end_bin_id, base_url, dataset, instrument, prefix, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  pid   max_value_column  max_value      date\n",
      "0      D20240215T152410_IFCB010_00001              fiber     1.0000  20240215\n",
      "1      D20240215T152410_IFCB010_00002              fiber     1.0000  20240215\n",
      "2      D20240215T152410_IFCB010_00004   nanoplankton_mix     0.9920  20240215\n",
      "3      D20240215T152410_IFCB010_00005  Bacillariophyceae     0.2783  20240215\n",
      "4      D20240215T152410_IFCB010_00006              fiber     0.9850  20240215\n",
      "...                               ...                ...        ...       ...\n",
      "71019  D20240215T191619_IFCB010_06417        Cryptophyta     0.8765  20240215\n",
      "71020  D20240215T191619_IFCB010_06418   nanoplankton_mix     0.9960  20240215\n",
      "71021  D20240215T191619_IFCB010_06419              fiber     0.9940  20240215\n",
      "71022  D20240215T191619_IFCB010_06421              fiber     0.9604  20240215\n",
      "71023  D20240215T191619_IFCB010_06422   nanoplankton_mix     0.9985  20240215\n",
      "\n",
      "[71024 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#STEP 2.1, SPATIAL GROUPING FOR ONE DAY\n",
    "#Choose the group with highest score for eVery images\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "if_files_root = r\"../../data/mvco\" # this is the folder pathway to your local computer \n",
    "\n",
    "# List to store summary data for each file\n",
    "all_summaries = []\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for file_name in os.listdir(if_files_root):\n",
    "    if file_name.startswith('D2024') and file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(if_files_root, file_name)\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Keep the 'pid' column and process the numeric columns for max values\n",
    "        pid_column = df['pid']\n",
    "        numeric_df = df.iloc[:, 1:].fillna(-float('inf'))  # Process the numeric columns, excluding 'pid'\n",
    "        \n",
    "        # Find the max value and corresponding column\n",
    "        df['max_value_column'] = numeric_df.idxmax(axis=1)\n",
    "        df['max_value'] = numeric_df.max(axis=1)\n",
    "        \n",
    "        # Create a summary DataFrame, including 'pid', 'max_value_column', and 'max_value'\n",
    "        summary_df = pd.DataFrame({\n",
    "            'pid': pid_column,\n",
    "            'max_value_column': df['max_value_column'],\n",
    "            'max_value': df['max_value']\n",
    "        })\n",
    "        \n",
    "        # Extract the date from the file name and remove the leading \"D\"\n",
    "        summary_df['date'] = file_name.split('T')[0][1:]\n",
    "        \n",
    "        # Append the summary to the list\n",
    "        all_summaries.append(summary_df)\n",
    "\n",
    "# Concatenate all summary DataFrames into one\n",
    "final_summary = pd.concat(all_summaries, ignore_index=True)\n",
    "\n",
    "# Print the final summary (or save it to a file if needed)\n",
    "print(final_summary)\n",
    "\n",
    "# Optionally, save the final summary to a CSV file\n",
    "final_summary.to_csv('../../data/final_summary.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/mvco_2024.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Now load the data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m difcb=\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../../data/mvco_2024.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Convert the cleaned date strings to datetime objects\u001b[39;00m\n\u001b[32m      4\u001b[39m difcb[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(difcb[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m=\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/conda/envs/notebook/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../../data/mvco_2024.csv'"
     ]
    }
   ],
   "source": [
    "# Now load the data\n",
    "difcb=pd.read_csv(\"../../data/mvco_2024.csv\")\n",
    "# Convert the cleaned date strings to datetime objects\n",
    "difcb['date'] = pd.to_datetime(difcb['date'], format='%Y%m%d')\n",
    "difcb.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now go to Hypercoast notebook\n",
    "\n",
    "To see how they got some PACE with hypercoast package--if you want."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
